{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46845c9f-bcb1-4484-90d1-49208995f7e9",
   "metadata": {},
   "source": [
    "<h1 style='text-align: center;'> Part B - Tweets Fine-tuning, model training and compression with comparison </h1>\n",
    "<h3 style='text-align: center;'> Group T, IDs: 316398387 ,318481447</h3>\n",
    "\n",
    "Based on the preprocessing phase done in the previous section, we will approach this with 2 models, one is **Encoder(only) based** model, and the other will be **Decoder only**\n",
    "<h6 style='text-align: left;'>similar imports like previous part:</h6>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9fb07fe-98df-4df0-b99d-b53bc8a5155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "import seaborn as sns\n",
    "from decouple import Config, RepositoryEnv \n",
    "import emoji\n",
    "from langdetect import detect, DetectorFactory\n",
    "from ftfy import fix_text\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import wandb\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "wandb.login(key=Config(RepositoryEnv(\"./.env\")).get('wandb_api_key'));  # If not necessary comment this line of W&B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c2cf426-1cb5-4fc1-ba88-65e70b0cecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"preprocessed_tweets.csv\"); df.head();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8544b68-ed6c-4af2-babe-f24f334204b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28609, 2), (7356, 2), (4905, 2))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLASS_TEXTS = ['extremely negative','negative','neutral','positive','extremely positive']  # order matters\n",
    "LABEL2ID = {s:i for i,s in enumerate(CLASS_TEXTS)}\n",
    "ID2LABEL = {i:s for s,i in LABEL2ID.items()}\n",
    "\n",
    "train_df, tmp = train_test_split(df, test_size=0.3, stratify=df['Sentiment'], random_state=42)\n",
    "val_df, test_df = train_test_split(tmp, test_size=0.4, stratify=tmp['Sentiment'], random_state=42)\n",
    "train_df.shape, val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a136797-2077-475a-99fb-35ec03fad6a9",
   "metadata": {},
   "source": [
    "### Decoder-only: TinlyLLAMA model\n",
    "\n",
    "We will start with the decoder only model where our samples need to be in a format of instruction. \n",
    "we will train the decoder in a generative way and its format will be like: \n",
    "\"Tweet: {text}\\nSentiment: {label_text}\" and we will mask the prompt part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73ad5818-4946-41ee-9630-9101daa8a72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c7c7e88d474ec38b77dab1a85172e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28609 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9898be97fab14d0a8f45e433cf017223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7356 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac15def6ec749af87176d8d8fd505ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4905 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # swap to Mistral/Llama-3 if you have VRAM\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "tokenizer.padding_side = \"right\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def build_example(text, label_text, max_in=160, max_label=4):\n",
    "    prompt = f\"Tweet: {text}\\nSentiment:\"\n",
    "    target = f\" {label_text}\"\n",
    "    enc_prompt = tokenizer(prompt, add_special_tokens=False, truncation=True, max_length=max_in)\n",
    "    enc_label  = tokenizer(target, add_special_tokens=False, truncation=True, max_length=max_label)\n",
    "\n",
    "    input_ids = enc_prompt['input_ids'] + enc_label['input_ids']\n",
    "    attn_mask = [1]*len(input_ids)\n",
    "    labels    = [-100]*len(enc_prompt['input_ids']) + enc_label['input_ids']\n",
    "    return {'input_ids':input_ids,'attention_mask':attn_mask,'labels':labels}\n",
    "\n",
    "import datasets\n",
    "\n",
    "def df_to_hf(df_):\n",
    "    return datasets.Dataset.from_pandas(df_[['clean_text','Sentiment']])\n",
    "\n",
    "hf_train = df_to_hf(train_df).map(lambda ex: build_example(ex['clean_text'], ex['Sentiment']), remove_columns=['clean_text','Sentiment'])\n",
    "hf_val   = df_to_hf(val_df).map(lambda ex: build_example(ex['clean_text'], ex['Sentiment']), remove_columns=['clean_text','Sentiment'])\n",
    "hf_test  = df_to_hf(test_df).map(lambda ex: build_example(ex['clean_text'], ex['Sentiment']), remove_columns=['clean_text','Sentiment'])\n",
    "\n",
    "data_collator = lambda batch: tokenizer.pad(batch, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2da35078-b36b-4e17-9899-af4d3ef8eac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # being used as a decorator instead of with torch.no_grad() knowd method it is a little bit more pythonic \n",
    "def score_labels_batch(model, tokenizer, texts, candidate_texts=CLASS_TEXTS, max_in=160, device=\"cuda\"):\n",
    "    prompt_batch = [f\"Tweet: {t}\\nSentiment:\" for t in texts]\n",
    "    enc = tokenizer(prompt_batch, padding=True, truncation=True, max_length=max_in, return_tensors=\"pt\").to(device)\n",
    "    scores = []\n",
    "    for lab in candidate_texts:\n",
    "        lab_ids = tokenizer(\" \"+lab, add_special_tokens=False, return_tensors=\"pt\")['input_ids'][0].to(device)\n",
    "        # teacher-forced logprob of label tokens\n",
    "        # run once per label with concatenated inputs\n",
    "        input_ids = torch.cat([enc.input_ids, lab_ids.repeat(len(texts),1)], dim=1)\n",
    "        attn_mask = torch.cat([enc.attention_mask, torch.ones((len(texts), lab_ids.size(0)), device=device, dtype=enc.attention_mask.dtype)], dim=1)\n",
    "        out = model(input_ids=input_ids, attention_mask=attn_mask)\n",
    "        logits = out.logits[:, -lab_ids.size(0)-1:-1, :]  # logits aligned to predict each label token\n",
    "        logp = F.log_softmax(logits, dim=-1)\n",
    "        token_logps = logp.gather(-1, lab_ids.view(1,-1).expand(len(texts),-1).unsqueeze(-1)).squeeze(-1).sum(dim=1)\n",
    "        scores.append(token_logps)\n",
    "    scores = torch.stack(scores, dim=1)  # [Batch, num_labels]\n",
    "    preds = scores.argmax(dim=1).cpu().tolist()\n",
    "    return [ID2LABEL[i] for i in preds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "98535626-f209-4774-b116-dd3783ddd61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd7a81d8c78a4208a1fbdbaddedf3678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "The installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. You may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m train_ds = TweetGenClsDS(train_df); val_ds = TweetGenClsDS(val_df)\n\u001b[32m     10\u001b[39m collate = data_collator\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m base = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m model = prepare_model_for_kbit_training(base)\n\u001b[32m     18\u001b[39m model = get_peft_model(model, LoraConfig(r=\u001b[32m16\u001b[39m, lora_alpha=\u001b[32m32\u001b[39m, lora_dropout=\u001b[32m0.05\u001b[39m, target_modules=[\u001b[33m\"\u001b[39m\u001b[33mq_proj\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mk_proj\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mv_proj\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mo_proj\u001b[39m\u001b[33m\"\u001b[39m], bias=\u001b[33m\"\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m\"\u001b[39m, task_type=\u001b[33m\"\u001b[39m\u001b[33mCAUSAL_LM\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_projects/group_T_ADL/group_T_ADL/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:600\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    599\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    604\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    605\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    606\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_projects/group_T_ADL/group_T_ADL/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:317\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    315\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_projects/group_T_ADL/group_T_ADL/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4887\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4884\u001b[39m     hf_quantizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4886\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4887\u001b[39m     \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4888\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4889\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4890\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4891\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4892\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4893\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4894\u001b[39m     torch_dtype = hf_quantizer.update_torch_dtype(torch_dtype)\n\u001b[32m   4895\u001b[39m     device_map = hf_quantizer.update_device_map(device_map)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_projects/group_T_ADL/group_T_ADL/.venv/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:88\u001b[39m, in \u001b[36mBnb4BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m version.parse(importlib.metadata.version(\u001b[33m\"\u001b[39m\u001b[33mbitsandbytes\u001b[39m\u001b[33m\"\u001b[39m)) < version.parse(\u001b[33m\"\u001b[39m\u001b[33m0.43.1\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.cuda.is_available():\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     89\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     90\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     91\u001b[39m         )\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_bnb_backend_availability\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n",
      "\u001b[31mImportError\u001b[39m: The installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. You may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1."
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True)\n",
    "\n",
    "class TweetGenClsDS(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.items = [build_example(t, y) for t,y in zip(df['clean_text'], df['Sentiment'])]\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __getitem__(self, i): return self.items[i]\n",
    "\n",
    "train_ds = TweetGenClsDS(train_df); val_ds = TweetGenClsDS(val_df)\n",
    "collate = data_collator\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else None\n",
    ")\n",
    "model = prepare_model_for_kbit_training(base)\n",
    "model = get_peft_model(model, LoraConfig(r=16, lora_alpha=32, lora_dropout=0.05, target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"], bias=\"none\", task_type=\"CAUSAL_LM\"))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=8, shuffle=True, collate_fn=collate)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=8, shuffle=False, collate_fn=collate)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=0.1)\n",
    "num_steps = len(train_loader)*2\n",
    "sched = get_cosine_schedule_with_warmup(opt, int(0.06*num_steps), num_steps)\n",
    "\n",
    "wandb.init(project=\"tweets-sentiment\", name=\"decoder_raw_loop\")\n",
    "model.train()\n",
    "for epoch in range(2):\n",
    "    for step, batch in enumerate(train_loader, 1):\n",
    "        batch = {k:v.to(model.device) for k,v in batch.items()}\n",
    "        out = model(**batch)\n",
    "        loss = out.loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step(); sched.step(); model.zero_grad()\n",
    "        if step % 50 == 0:\n",
    "            wandb.log({\"train/loss\": loss.item(), \"epoch\": epoch + step/len(train_loader)})\n",
    "    # quick val loss\n",
    "    model.eval(); val_loss = 0.0; n=0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k:v.to(model.device) for k,v in batch.items()}\n",
    "            val_loss += model(**batch).loss.item(); n+=1\n",
    "    wandb.log({\"val/loss\": val_loss/max(n,1), \"epoch\": epoch+1})\n",
    "    model.train()\n",
    "# test evaluation\n",
    "model.eval()\n",
    "preds = []\n",
    "truth = test_df['label_text'].tolist()\n",
    "texts = test_df['clean_text'].tolist()\n",
    "for i in range(0, len(texts), 64):\n",
    "    preds += score_labels_batch(model, tokenizer, texts[i:i+64], device=device)\n",
    "\n",
    "print(classification_report(truth, preds, digits=3, labels=CLASS_TEXTS))\n",
    "wandb.finish()\n",
    "\n",
    "def objective_hf(trial):\n",
    "    lr   = trial.suggest_float(\"lr\", 5e-5, 2e-4, log=True)\n",
    "    r    = trial.suggest_categorical(\"lora_r\", [8,16,32])\n",
    "    drop = trial.suggest_float(\"lora_dropout\", 0.0, 0.1)\n",
    "\n",
    "    base = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\", quantization_config=bnb_config, torch_dtype=torch.bfloat16 if torch.cuda.is_available() else None)\n",
    "    model = prepare_model_for_kbit_training(base)\n",
    "    peft_cfg = LoraConfig(r=r, lora_alpha=2*r, lora_dropout=drop, target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"], bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "    model = get_peft_model(model, peft_cfg)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"./runs/optuna_{trial.number}\",\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=2,\n",
    "        learning_rate=lr,\n",
    "        num_train_epochs=1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.06,\n",
    "        logging_steps=100,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        bf16=torch.cuda.is_available(),\n",
    "        report_to=[\"wandb\"],\n",
    "        run_name=f\"decoder_hf_trial_{trial.number}\"\n",
    "    )\n",
    "    wandb.init(project=\"tweets-sentiment\", name=f\"decoder_hf_trial_{trial.number}\", group=\"optuna_hf\", reinit=True)\n",
    "\n",
    "    trainer = Trainer(model=model, args=args, train_dataset=hf_train, eval_dataset=hf_val, data_collator=data_collator)\n",
    "    trainer.train()\n",
    "\n",
    "    # macro-F1 on val via label scoring\n",
    "    preds=[]; truth = val_df['label_text'].tolist(); texts = val_df['clean_text'].tolist()\n",
    "    for i in range(0,len(texts),64):\n",
    "        preds += score_labels_batch(model, tokenizer, texts[i:i+64], device=device)\n",
    "    macro_f1 = f1_score(truth, preds, average=\"macro\")\n",
    "    wandb.run.summary[\"val_macro_f1\"] = macro_f1\n",
    "    wandb.finish()\n",
    "    return macro_f1\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective_hf, n_trials=10)\n",
    "print(\"Best:\", study.best_value, study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e048ee4-efda-46cf-a12b-1ddca6cb76a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01c7dc7dca27491ba4315951d536ddc5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "047a1186f5924f139e38591683e807f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "05bc6c6e43474c22b1e0d05919b388fa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0a54c1e1104a48d1a44bb6213ee97686": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0b5846d07b0a40f09ae7b7807fe2f8b8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0f1b6fec64ec49ff94838910dca2ec0b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "20px"
      }
     },
     "12109f7556df4c999f08c3c360a9e001": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "1532dbdff0a14110bea2e49b3c208969": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_de645524dde34a9b8f69592db8338c36",
       "style": "IPY_MODEL_fa9d6dbe395f49b88e4ca7814aa28d0d",
       "value": " 608/608 [00:00&lt;00:00, 62.3kB/s]"
      }
     },
     "23b4ffc80f4f4998b723c7961568132a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3068cc511d654776baec2479326b275d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3295092c16c5426ea831f3e36409b980": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "360e0ffa23fd4b7e81ae3abf520822b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3c723c8e106545c58807bc345e62a67b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f5b02d66f4c24bd483fcfdc8866d8f87",
       "style": "IPY_MODEL_9ff946b0d1b14c219ad6b1254aefe1c3",
       "value": "tokenizer.json: "
      }
     },
     "3e796aaafeb8495981b3d0da8c14ac8a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "42c7c7e88d474ec38b77dab1a85172e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_d75aae9e6edc416f8404398134a22237",
        "IPY_MODEL_44e54b23db7e42acb13b66820588a64b",
        "IPY_MODEL_ef7e92ebf6064f8bbdcef4385aab3441"
       ],
       "layout": "IPY_MODEL_94dad48ad72949f880af7459bebb5d65"
      }
     },
     "44e54b23db7e42acb13b66820588a64b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_0b5846d07b0a40f09ae7b7807fe2f8b8",
       "max": 28609,
       "style": "IPY_MODEL_79ca1a3d69ac405a991cba01ec921c1d",
       "value": 28609
      }
     },
     "45e39ed1e9c8478fa6d62fe4402a7aea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_cd780fa329c24a2dbc9363a1f3535dda",
       "max": 551,
       "style": "IPY_MODEL_12109f7556df4c999f08c3c360a9e001",
       "value": 551
      }
     },
     "46ca362ad70d45ffa21aa0d4ea1a70cd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d0c766d1f39b42ea901a123f2659eb1c",
       "style": "IPY_MODEL_0a54c1e1104a48d1a44bb6213ee97686",
       "value": "special_tokens_map.json: 100%"
      }
     },
     "4806b939cc0449268f697b01712fbdeb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_6fcec12994e744a4b5c1a2e473fa7d5a",
       "style": "IPY_MODEL_ca9f4b2ef92048ecb8f627066eeee57b",
       "value": " 7356/7356 [00:00&lt;00:00, 8936.49 examples/s]"
      }
     },
     "4ab07971e02d4051a25b07201920ba29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "4d2cf3e1cd4549a684e3e21d3ef5357d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4ebcc08f08ca4ec2910a653ccbbeba56": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "510a0e0a72cf4bd5ab8efe9191f0613f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5240f229b0d64c80ade2d16757de6215": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "53b26bb1401e4ab69748fbca46fb8415": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5965d4be9e8b4149875361165ce63eba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5a694201a1344518a3e6f4b2544da101": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_46ca362ad70d45ffa21aa0d4ea1a70cd",
        "IPY_MODEL_45e39ed1e9c8478fa6d62fe4402a7aea",
        "IPY_MODEL_e19dbe7cedc64acf8db9119c53892826"
       ],
       "layout": "IPY_MODEL_6646e8f3ae4c4f649e70c3fbbbbac01a"
      }
     },
     "5ac15def6ec749af87176d8d8fd505ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ae582cc4661c4b088ccee7caa42c03f0",
        "IPY_MODEL_feaa69575421405b9af4ef3da22cb4d6",
        "IPY_MODEL_8d3ae75fe2db46338c8e261ed8ca5227"
       ],
       "layout": "IPY_MODEL_df74245c59a140fcb51fb0723ce5694c"
      }
     },
     "5c8e5042562a456a9a43b1b1e78e85f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3068cc511d654776baec2479326b275d",
       "style": "IPY_MODEL_c30e840be8e64569a38ff038f8704cf8",
       "value": " 500k/500k [00:01&lt;00:00, 330kB/s]"
      }
     },
     "5df9f571941546f59b9f05f61dd4b35c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_89e10d95187146d298aea062de2299fa",
       "max": 1,
       "style": "IPY_MODEL_f3efb0bef2514342890d2e558971784d",
       "value": 1
      }
     },
     "6646e8f3ae4c4f649e70c3fbbbbac01a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6b73e4a4d7664f788289c7be3360576e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_c3fddd73b0dd472b8957e1dd49af4996",
       "max": 608,
       "style": "IPY_MODEL_79f5b2bc4f6c45bdb07aafef64eb3cad",
       "value": 608
      }
     },
     "6c8c1e0d2894425790f77b9ff9ecb864": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_53b26bb1401e4ab69748fbca46fb8415",
       "style": "IPY_MODEL_97a01469f7c245e8b16f40c0882f0b73",
       "value": "tokenizer.model: 100%"
      }
     },
     "6fcec12994e744a4b5c1a2e473fa7d5a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7673016edcef4ff88c14add54643d958": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_fa8cfd5b4dee426fa40ee740c29a6ce0",
       "style": "IPY_MODEL_b676f10284c04bc3b7c892c484fb0eaf",
       "value": "config.json: 100%"
      }
     },
     "79ca1a3d69ac405a991cba01ec921c1d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "79f5b2bc4f6c45bdb07aafef64eb3cad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "7da1eb9bc0154a73881c1aea52b65149": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "807d23a509bb49f984ccd1da65453a97": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_af54581a5864464dad08eb5539e454be",
       "style": "IPY_MODEL_4d2cf3e1cd4549a684e3e21d3ef5357d",
       "value": " 1.84M/? [00:00&lt;00:00, 40.8MB/s]"
      }
     },
     "83e20f93eb7d43b0ae60fe63eec16896": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "88f648a7873c41ff96319a2d7e5ef7ac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ec141df2ab65443d99fa6c12a9f9ec25",
        "IPY_MODEL_5df9f571941546f59b9f05f61dd4b35c",
        "IPY_MODEL_8b52afeef92545f5873e5534a285d1df"
       ],
       "layout": "IPY_MODEL_3e796aaafeb8495981b3d0da8c14ac8a"
      }
     },
     "89e10d95187146d298aea062de2299fa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "20px"
      }
     },
     "8b52afeef92545f5873e5534a285d1df": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_23b4ffc80f4f4998b723c7961568132a",
       "style": "IPY_MODEL_5240f229b0d64c80ade2d16757de6215",
       "value": " 1.29k/? [00:00&lt;00:00, 80.7kB/s]"
      }
     },
     "8d29fdab0c6a4e7ab9eeeab28add236e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8d3ae75fe2db46338c8e261ed8ca5227": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_83e20f93eb7d43b0ae60fe63eec16896",
       "style": "IPY_MODEL_90f8b9fa9b344d86ac80c6b601dfd1e3",
       "value": " 4905/4905 [00:00&lt;00:00, 8880.10 examples/s]"
      }
     },
     "8dcf8a127dd6461cb740ff4881fa4989": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "8e69a3182b164856a3e4f28f91a8ccc3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "90f8b9fa9b344d86ac80c6b601dfd1e3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "92e7aace68314bf5926305f29fd2e9cc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "945442594c424fa3b5647e1ad71a2fee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_3c723c8e106545c58807bc345e62a67b",
        "IPY_MODEL_f7b3405c5e0e41c88412248c8c49231a",
        "IPY_MODEL_807d23a509bb49f984ccd1da65453a97"
       ],
       "layout": "IPY_MODEL_f47f126a194e4855a27a3e151d634f86"
      }
     },
     "94dad48ad72949f880af7459bebb5d65": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "960bd243d7874c44a6ca77267c708733": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e6302579f72a4ae285194156e658e102",
       "style": "IPY_MODEL_b5a16008516b440ebe0a713b3fdeb1ee",
       "value": "Map: 100%"
      }
     },
     "97a01469f7c245e8b16f40c0882f0b73": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9898be97fab14d0a8f45e433cf017223": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_960bd243d7874c44a6ca77267c708733",
        "IPY_MODEL_b0884f636e69450ea75f63859a32a514",
        "IPY_MODEL_4806b939cc0449268f697b01712fbdeb"
       ],
       "layout": "IPY_MODEL_ca7c41111e7546c89b552d7fab53fb3e"
      }
     },
     "9df55266042e4bf9ade8a38ab1b680e0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9ff946b0d1b14c219ad6b1254aefe1c3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "aadb7f2e1e6246dba9b89697824d691e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "ae582cc4661c4b088ccee7caa42c03f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4ebcc08f08ca4ec2910a653ccbbeba56",
       "style": "IPY_MODEL_3295092c16c5426ea831f3e36409b980",
       "value": "Map: 100%"
      }
     },
     "af54581a5864464dad08eb5539e454be": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b0884f636e69450ea75f63859a32a514": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_8e69a3182b164856a3e4f28f91a8ccc3",
       "max": 7356,
       "style": "IPY_MODEL_e4d4d281ada541f487be03e954a3ea16",
       "value": 7356
      }
     },
     "b5a16008516b440ebe0a713b3fdeb1ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b676f10284c04bc3b7c892c484fb0eaf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c30e840be8e64569a38ff038f8704cf8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c3fddd73b0dd472b8957e1dd49af4996": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ca5b4e8626fc407981f081468ab0ab48": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_6c8c1e0d2894425790f77b9ff9ecb864",
        "IPY_MODEL_cba732cc5041492992f9970cc046914b",
        "IPY_MODEL_5c8e5042562a456a9a43b1b1e78e85f9"
       ],
       "layout": "IPY_MODEL_5965d4be9e8b4149875361165ce63eba"
      }
     },
     "ca7c41111e7546c89b552d7fab53fb3e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ca9f4b2ef92048ecb8f627066eeee57b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "cba732cc5041492992f9970cc046914b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_d2eb2da2d8d14bc6aed2cb7b9c60e03c",
       "max": 499723,
       "style": "IPY_MODEL_aadb7f2e1e6246dba9b89697824d691e",
       "value": 499723
      }
     },
     "cd780fa329c24a2dbc9363a1f3535dda": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ce7701f6db684e7886e44beb4ca22ec1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d0c766d1f39b42ea901a123f2659eb1c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d2eb2da2d8d14bc6aed2cb7b9c60e03c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d75aae9e6edc416f8404398134a22237": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9df55266042e4bf9ade8a38ab1b680e0",
       "style": "IPY_MODEL_01c7dc7dca27491ba4315951d536ddc5",
       "value": "Map: 100%"
      }
     },
     "dd7a81d8c78a4208a1fbdbaddedf3678": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_7673016edcef4ff88c14add54643d958",
        "IPY_MODEL_6b73e4a4d7664f788289c7be3360576e",
        "IPY_MODEL_1532dbdff0a14110bea2e49b3c208969"
       ],
       "layout": "IPY_MODEL_92e7aace68314bf5926305f29fd2e9cc"
      }
     },
     "de645524dde34a9b8f69592db8338c36": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "df74245c59a140fcb51fb0723ce5694c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e19dbe7cedc64acf8db9119c53892826": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_ce7701f6db684e7886e44beb4ca22ec1",
       "style": "IPY_MODEL_360e0ffa23fd4b7e81ae3abf520822b9",
       "value": " 551/551 [00:00&lt;00:00, 31.1kB/s]"
      }
     },
     "e4d4d281ada541f487be03e954a3ea16": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "e6302579f72a4ae285194156e658e102": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ec141df2ab65443d99fa6c12a9f9ec25": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_05bc6c6e43474c22b1e0d05919b388fa",
       "style": "IPY_MODEL_510a0e0a72cf4bd5ab8efe9191f0613f",
       "value": "tokenizer_config.json: "
      }
     },
     "ef7e92ebf6064f8bbdcef4385aab3441": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_8d29fdab0c6a4e7ab9eeeab28add236e",
       "style": "IPY_MODEL_047a1186f5924f139e38591683e807f2",
       "value": " 28609/28609 [00:03&lt;00:00, 9014.12 examples/s]"
      }
     },
     "f3efb0bef2514342890d2e558971784d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f47f126a194e4855a27a3e151d634f86": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f5b02d66f4c24bd483fcfdc8866d8f87": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f7b3405c5e0e41c88412248c8c49231a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_0f1b6fec64ec49ff94838910dca2ec0b",
       "max": 1,
       "style": "IPY_MODEL_4ab07971e02d4051a25b07201920ba29",
       "value": 1
      }
     },
     "fa8cfd5b4dee426fa40ee740c29a6ce0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fa9d6dbe395f49b88e4ca7814aa28d0d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "feaa69575421405b9af4ef3da22cb4d6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_7da1eb9bc0154a73881c1aea52b65149",
       "max": 4905,
       "style": "IPY_MODEL_8dcf8a127dd6461cb740ff4881fa4989",
       "value": 4905
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
