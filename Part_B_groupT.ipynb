{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46845c9f-bcb1-4484-90d1-49208995f7e9",
   "metadata": {},
   "source": [
    "<h1 style='text-align: center;'> Part B - Tweets Fine-tuning, model training and compression with comparison </h1>\n",
    "<h3 style='text-align: center;'> Group T, IDs: 316398387 ,318481447</h3>\n",
    "\n",
    "Based on the preprocessing phase done in the previous section, we will approach this with 2 models, one is **Encoder(only) based** model, and the other will be **Decoder only**\n",
    "<h6 style='text-align: left;'>similar imports like previous part:</h6>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b9fb07fe-98df-4df0-b99d-b53bc8a5155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig, AutoTokenizer, get_linear_schedule_with_warmup, AutoModelForSequenceClassification\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import seaborn as sns\n",
    "from decouple import Config, RepositoryEnv \n",
    "import emoji\n",
    "import optuna\n",
    "from langdetect import detect, DetectorFactory\n",
    "from ftfy import fix_text\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import wandb\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import re\n",
    "import warnings\n",
    "import optuna\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "wandb.login(key=Config(RepositoryEnv(\"./.env\")).get('wandb_api_key'));  # If not necessary comment this line of W&B, or change to wandb.login() and pase API key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1c2cf426-1cb5-4fc1-ba88-65e70b0cecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"preprocessed_tweets.csv\"); df.head(); # The preprocessed tweets csv should be in same root folder after cloning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b8544b68-ed6c-4af2-babe-f24f334204b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28609, 2), (7356, 2), (4905, 2))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLASS_TEXTS = ['extremely negative','negative','neutral','positive','extremely positive']  # order matters\n",
    "LABEL2ID = {s:i for i,s in enumerate(CLASS_TEXTS)}\n",
    "ID2LABEL = {i:s for s,i in LABEL2ID.items()}\n",
    "\n",
    "train_df, tmp = train_test_split(df, test_size=0.3, stratify=df['Sentiment'], random_state=42)\n",
    "val_df, test_df = train_test_split(tmp, test_size=0.4, stratify=tmp['Sentiment'], random_state=42)\n",
    "train_df.shape, val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a136797-2077-475a-99fb-35ec03fad6a9",
   "metadata": {},
   "source": [
    "### Decoder-only: TinlyLLAMA model\n",
    "\n",
    "We will start with the decoder only model where our samples need to be in a format of instruction. \n",
    "we will train the decoder in a generative way and its format will be like: \n",
    "\"Tweet: {text}\\nSentiment: {label_text}\" and we will mask the prompt part.\n",
    "<br>\n",
    "The **Raw Pytorch fine-tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "73ad5818-4946-41ee-9630-9101daa8a72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # swap to Mistral/Llama-3 if you have VRAM\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "tokenizer.padding_side = \"right\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def build_prompt(text: str) -> str:\n",
    "    return f\"Tweet: {text}\\nSentiment:\"\n",
    "\n",
    "def build_target(label_text: str) -> str:\n",
    "    return label_text + tokenizer.eos_token\n",
    "\n",
    "class GenTweetSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=160):\n",
    "        self.prompts = [build_prompt(t) for t in df[\"clean_text\"]]\n",
    "        self.targets = [build_target(s) for s in df[\"Sentiment\"]]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self): return len(self.prompts)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        s_enc = self.tokenizer(self.prompts[i], add_special_tokens=False)\n",
    "        t_enc = self.tokenizer(self.targets[i], add_special_tokens=False)\n",
    "        input_ids  = s_enc[\"input_ids\"] + t_enc[\"input_ids\"]\n",
    "        attn_mask  = [1]*len(input_ids)\n",
    "        labels     = [-100]*len(s_enc[\"input_ids\"]) + t_enc[\"input_ids\"]\n",
    "        if len(input_ids) > self.max_len:\n",
    "            cut = self.max_len\n",
    "            # keep at least target portion\n",
    "            keep_src = max(0, cut - len(t_enc[\"input_ids\"]))\n",
    "            input_ids = input_ids[:keep_src] + t_enc[\"input_ids\"][:cut-keep_src]\n",
    "            attn_mask = [1]*len(input_ids)\n",
    "            labels    = [-100]*keep_src + t_enc[\"input_ids\"][:cut-keep_src]\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attn_mask, \"labels\": labels}\n",
    "\n",
    "def pad_collate(batch):\n",
    "    maxlen = max(len(x[\"input_ids\"]) for x in batch)\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    out = {\"input_ids\":[], \"attention_mask\":[], \"labels\":[]}\n",
    "    for x in batch:\n",
    "        pad = maxlen - len(x[\"input_ids\"])\n",
    "        out[\"input_ids\"].append(x[\"input_ids\"] + [pad_id]*pad)\n",
    "        out[\"attention_mask\"].append(x[\"attention_mask\"] + [0]*pad)\n",
    "        out[\"labels\"].append(x[\"labels\"] + [-100]*pad)\n",
    "    return {k: torch.tensor(v) for k,v in out.items()}\n",
    "\n",
    "def build_causal_lora(lora_r=16, lora_alpha=32, lora_dropout=0.05):\n",
    "    quant = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True,\n",
    "                               bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME, quantization_config=quant, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    "    )\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    peft = LoraConfig(\n",
    "        r=lora_r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    model = get_peft_model(model, peft)\n",
    "    return model\n",
    "\n",
    "def extract_label(gen_text: str):\n",
    "    s = gen_text.lower()\n",
    "    for lab in CLASS_TEXTS:\n",
    "        if lab.lower() in s:\n",
    "            return lab\n",
    "    # fallback heuristics\n",
    "    if \"very positive\" in s or \"extremely positive\" in s: return \"Extremely Positive\"\n",
    "    if \"very negative\" in s or \"extremely negative\" in s: return \"Extremely Negative\"\n",
    "    if \"positive\" in s: return \"Positive\"\n",
    "    if \"negative\" in s: return \"Negative\"\n",
    "    if \"neutral\" in s:  return \"Neutral\"\n",
    "    return \"Neutral\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_macro_f1(model, df_eval, max_len=160, max_new_tokens=4, bs=64):\n",
    "    prompts = [build_prompt(t) for t in df_eval[\"clean_text\"]]\n",
    "    gold    = df_eval[\"Sentiment\"].tolist()\n",
    "    preds = []\n",
    "    for i in range(0, len(prompts), bs):\n",
    "        batch = prompts[i:i+bs]\n",
    "        enc = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len).to(model.device)\n",
    "        out = model.generate(\n",
    "            **enc, max_new_tokens=max_new_tokens, do_sample=False, eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        input_lens = enc[\"attention_mask\"].sum(dim=1).tolist()\n",
    "        for j, ids in enumerate(out):\n",
    "            gen_ids = ids[input_lens[j]:]\n",
    "            text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "            preds.append(extract_label(text))\n",
    "    y_true = [LABEL2ID[x] for x in gold]\n",
    "    y_pred = [LABEL2ID.get(x, LABEL2ID[\"Neutral\"]) for x in preds]\n",
    "    return f1_score(y_true, y_pred, average=\"macro\")\n",
    "  \n",
    "\n",
    "def run_epoch(model, loader, optimizer=None, scheduler=None, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "    total_loss = 0.0\n",
    "    for batch in loader:\n",
    "        for k in batch: batch[k] = batch[k].to(model.device)\n",
    "        with torch.set_grad_enabled(train):\n",
    "            out = model(**batch)\n",
    "            loss = out.loss\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                if scheduler: scheduler.step()\n",
    "        total_loss += loss.item() * batch[\"input_ids\"].size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def objective_raw(trial: optuna.Trial):\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
    "    weight_decay  = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-4)\n",
    "    patience      = trial.suggest_int(\"patience\", 7, 10)\n",
    "    batch_size    = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    num_layers    = trial.suggest_int(\"num_layers\", 1, 3)  # placeholder\n",
    "    max_len       = trial.suggest_categorical(\"max_length\", [128, 160, 192])\n",
    "    num_epochs    = trial.suggest_int(\"num_epochs\", 2, 4)\n",
    "    warmup_ratio  = trial.suggest_float(\"warmup_ratio\", 0.05, 0.2)\n",
    "    lora_r        = trial.suggest_categorical(\"lora_r\", [8, 16, 32])\n",
    "    lora_alpha    = trial.suggest_categorical(\"lora_alpha\", [16, 32, 64])\n",
    "    lora_dropout  = trial.suggest_float(\"lora_dropout\", 0.0, 0.2)\n",
    "\n",
    "    wandb_run = wandb.init(project=\"tweets-decoder-gentask-raw\", config=trial.params)\n",
    "\n",
    "    model = build_causal_lora(lora_r, lora_alpha, lora_dropout)\n",
    "\n",
    "    tr_ds = GenTweetSet(train_df, tokenizer, max_len=max_len)\n",
    "    va_ds = GenTweetSet(val_df, tokenizer, max_len=max_len)\n",
    "    tr_ld = DataLoader(tr_ds, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
    "    va_ld = DataLoader(va_ds, batch_size=batch_size, shuffle=False, collate_fn=pad_collate)\n",
    "\n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    steps = num_epochs * len(tr_ld)\n",
    "    warmup = int(warmup_ratio * steps)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, warmup, steps)\n",
    "\n",
    "    best_f1, no_improve = -1.0, 0\n",
    "    for ep in range(1, num_epochs+1):\n",
    "        tr_loss = run_epoch(model, tr_ld, optimizer, scheduler, train=True)\n",
    "        va_loss = run_epoch(model, va_ld, train=False)\n",
    "        va_f1 = eval_macro_f1(model, val_df, max_len=max_len)\n",
    "        wandb.log({\"epoch\": ep, \"train_loss\": tr_loss, \"val_loss\": va_loss, \"val_macro_f1\": va_f1})\n",
    "        trial.report(va_f1, ep)\n",
    "        if trial.should_prune(): raise optuna.TrialPruned()\n",
    "        if va_f1 > best_f1:\n",
    "            best_f1, no_improve = va_f1, 0\n",
    "            model.save_pretrained(f\"best_raw_gen_trial_{trial.number}\")\n",
    "            tokenizer.save_pretrained(f\"best_raw_gen_trial_{trial.number}\")\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                break\n",
    "\n",
    "    wandb.summary[\"best_val_macro_f1\"] = best_f1\n",
    "    wandb_run.finish()\n",
    "    return best_f1\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective_raw, n_trials=8)\n",
    "print(\"Best raw macro-F1:\", study.best_value, study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6bbda0-67e4-4a8a-8d92-8e1364936ad7",
   "metadata": {},
   "source": [
    "**The HF trainer** + optuna and W&B experiment like the above raw training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bfe67c00-34bd-4b0f-ba62-9ef4ac211a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def hf_build_decoder_ds(df, max_len=160):\n",
    "    # map into prompt -> labels (causal LM): mask source tokens with -100\n",
    "    def _enc(ex):\n",
    "        src = build_prompt(ex[\"clean_text\"])\n",
    "        tgt = build_target(ex[\"Sentiment\"])\n",
    "        s = tokenizer(src, add_special_tokens=False)\n",
    "        t = tokenizer(tgt, add_special_tokens=False)\n",
    "        inp = s[\"input_ids\"] + t[\"input_ids\"]\n",
    "        att = [1]*len(inp)\n",
    "        lab = [-100]*len(s[\"input_ids\"]) + t[\"input_ids\"]\n",
    "        # truncate keeping all target tokens\n",
    "        if len(inp) > max_len:\n",
    "            keep_src = max(0, max_len - len(t[\"input_ids\"]))\n",
    "            inp = inp[:keep_src] + t[\"input_ids\"][:max_len-keep_src]\n",
    "            att = [1]*len(inp)\n",
    "            lab = [-100]*keep_src + t[\"input_ids\"][:max_len-keep_src]\n",
    "        return {\"input_ids\": inp, \"attention_mask\": att, \"labels\": lab}\n",
    "\n",
    "    cols = [\"input_ids\",\"attention_mask\",\"labels\"]\n",
    "    ds = Dataset.from_pandas(df[[\"clean_text\",\"Sentiment\"]])\n",
    "    ds = ds.map(_enc, remove_columns=[c for c in ds.column_names if c not in [\"clean_text\",\"Sentiment\"]])\n",
    "    ds = ds.remove_columns([\"clean_text\",\"Sentiment\"])\n",
    "    return ds\n",
    "\n",
    "def hf_decoder_collator(features):\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    maxlen = max(len(f[\"input_ids\"]) for f in features)\n",
    "    batch = {\"input_ids\":[], \"attention_mask\":[], \"labels\":[]}\n",
    "    for f in features:\n",
    "        pad = maxlen - len(f[\"input_ids\"])\n",
    "        batch[\"input_ids\"].append(f[\"input_ids\"] + [pad_id]*pad)\n",
    "        batch[\"attention_mask\"].append(f[\"attention_mask\"] + [0]*pad)\n",
    "        batch[\"labels\"].append(f[\"labels\"] + [-100]*pad)\n",
    "    return {k: torch.tensor(v) for k,v in batch.items()}\n",
    "\n",
    "# ---------- Trainer factory ----------\n",
    "\n",
    "def build_decoder_trainer(params):\n",
    "    max_len = params[\"max_length\"]\n",
    "    d_train = hf_build_decoder_ds(train_df, max_len=max_len)\n",
    "    d_val   = hf_build_decoder_ds(val_df,   max_len=max_len)\n",
    "\n",
    "    quant = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME, quantization_config=quant, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    "    )\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    peft_cfg = LoraConfig(\n",
    "        r=params[\"lora_r\"], lora_alpha=params[\"lora_alpha\"], lora_dropout=params[\"lora_dropout\"],\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    model = get_peft_model(model, peft_cfg)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"./hf_decoder_out\",\n",
    "        per_device_train_batch_size=params[\"batch_size\"],\n",
    "        per_device_eval_batch_size=params[\"batch_size\"],\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        weight_decay=params[\"weight_decay\"],\n",
    "        num_train_epochs=params[\"num_epochs\"],\n",
    "        warmup_ratio=params[\"warmup_ratio\"],\n",
    "        logging_strategy=\"steps\", logging_steps=50,\n",
    "        evaluation_strategy=\"steps\", eval_steps=200,\n",
    "        save_strategy=\"steps\", save_steps=200,\n",
    "        load_best_model_at_end=True, metric_for_best_model=\"eval_loss\",\n",
    "        gradient_checkpointing=True,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=[\"wandb\"]\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model, args=args,\n",
    "        train_dataset=d_train, eval_dataset=d_val,\n",
    "        data_collator=hf_decoder_collator, tokenizer=tokenizer\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "# ---------- Optuna objective for HF Trainer ----------\n",
    "\n",
    "def objective_hf_decoder(trial: optuna.Trial):\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3),\n",
    "        \"weight_decay\":  trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-4),\n",
    "        \"patience\":      trial.suggest_int(\"patience\", 7, 10),   # used only for raw; kept for parity\n",
    "        \"batch_size\":    trial.suggest_categorical(\"batch_size\", [32, 64, 128]),\n",
    "        \"num_layers\":    trial.suggest_int(\"num_layers\", 1, 3),  # placeholder (not used directly)\n",
    "        \"max_length\":    trial.suggest_categorical(\"max_length\", [128, 160, 192]),\n",
    "        \"num_epochs\":    trial.suggest_int(\"num_epochs\", 2, 4),\n",
    "        \"warmup_ratio\":  trial.suggest_float(\"warmup_ratio\", 0.05, 0.2),\n",
    "        \"lora_r\":        trial.suggest_categorical(\"lora_r\", [8, 16, 32]),\n",
    "        \"lora_alpha\":    trial.suggest_categorical(\"lora_alpha\", [16, 32, 64]),\n",
    "        \"lora_dropout\":  trial.suggest_float(\"lora_dropout\", 0.0, 0.2),\n",
    "    }\n",
    "\n",
    "    run = wandb.init(project=\"tweets-decoder-gentask-trainer\", config=params)\n",
    "    trainer = build_decoder_trainer(params)\n",
    "    trainer.train()\n",
    "\n",
    "    # compute macro-F1 via generation (same helper you already have)\n",
    "    val_f1 = eval_macro_f1(trainer.model, val_df, max_len=params[\"max_length\"])\n",
    "    wandb.summary[\"val_macro_f1_gen\"] = val_f1\n",
    "\n",
    "    # persist best\n",
    "    out_dir = f\"best_trainer_gen_trial_{trial.number}\"\n",
    "    trainer.save_model(out_dir); tokenizer.save_pretrained(out_dir)\n",
    "    run.finish()\n",
    "    return val_f1\n",
    "\n",
    "# Example separate study for HF:\n",
    "study_hf = optuna.create_study(direction=\"maximize\")\n",
    "study_hf.optimize(objective_hf_decoder, n_trials=2)\n",
    "print(\"Best HF (decoder) macro-F1:\", study_hf.best_value, study_hf.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5580bd0e-98d7-468a-9194-06a29952e09c",
   "metadata": {},
   "source": [
    "### Encoder-Only: DeBERTa-v3 based classification: \n",
    "\n",
    "In this part we will apply same process as in the decoder part but now on the encoder part so we can see what is more robust and performs better way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2f949e69-8d6e-47af-b85f-0861b1885ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Encoder (DeBERTa-v3) setup -----\n",
    "ENC_MODEL = \"microsoft/deberta-v3-base\"\n",
    "enc_tokenizer = AutoTokenizer.from_pretrained(ENC_MODEL, use_fast=True)\n",
    "\n",
    "LABELS = CLASS_TEXTS  # [\"Extremely Negative\",\"Negative\",\"Neutral\",\"Positive\",\"Extremely Positive\"]\n",
    "ID2LABEL = {i:lab for i,lab in enumerate(LABELS)}\n",
    "LABEL2ID = {lab:i for i,lab in enumerate(LABELS)}\n",
    "\n",
    "class EncTweetSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=160):\n",
    "        self.texts = df[\"clean_text\"].tolist()\n",
    "        self.labels = [LABEL2ID[s] for s in df[\"Sentiment\"].tolist()]\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, i):\n",
    "        enc = self.tok(self.texts[i], truncation=True, max_length=self.max_len)\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"],\n",
    "            \"attention_mask\": enc[\"attention_mask\"],\n",
    "            \"labels\": self.labels[i]\n",
    "        }\n",
    "\n",
    "def enc_pad_collate(batch):\n",
    "    pad_id = enc_tokenizer.pad_token_id\n",
    "    maxlen = max(len(x[\"input_ids\"]) for x in batch)\n",
    "    out = {\"input_ids\":[], \"attention_mask\":[], \"labels\":[]}\n",
    "    for x in batch:\n",
    "        pad = maxlen - len(x[\"input_ids\"])\n",
    "        out[\"input_ids\"].append(x[\"input_ids\"] + [pad_id]*pad)\n",
    "        out[\"attention_mask\"].append(x[\"attention_mask\"] + [0]*pad)\n",
    "        out[\"labels\"].append(x[\"labels\"])\n",
    "    return {k: torch.tensor(v) for k,v in out.items()}\n",
    "\n",
    "def build_deberta_classif(num_labels=5, lora_cfg=None):\n",
    "    quant = BitsAndBytesConfig(load_in_8bit=True)  # 8-bit encoder is usually fine; or fp16 if you prefer\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        ENC_MODEL, num_labels=num_labels, id2label=ID2LABEL, label2id=LABEL2ID,\n",
    "        quantization_config=quant, device_map=\"auto\"\n",
    "    )\n",
    "    if lora_cfg is not None:\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        model = get_peft_model(model, lora_cfg)\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def enc_eval_macro_f1(model, loader):\n",
    "    model.eval()\n",
    "    preds, gold = [], []\n",
    "    for batch in loader:\n",
    "        for k in batch: batch[k] = batch[k].to(model.device)\n",
    "        out = model(**batch)\n",
    "        p = out.logits.argmax(dim=-1).detach().cpu().tolist()\n",
    "        y = batch[\"labels\"].detach().cpu().tolist()\n",
    "        preds.extend(p); gold.extend(y)\n",
    "    return f1_score(gold, preds, average=\"macro\")\n",
    "\n",
    "# ---------- RAW PyTorch + Optuna for DeBERTa ----------\n",
    "\n",
    "def objective_enc_raw(trial: optuna.Trial):\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 5e-4)\n",
    "    weight_decay  = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-4)\n",
    "    patience      = trial.suggest_int(\"patience\", 5, 10)\n",
    "    batch_size    = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "    max_len       = trial.suggest_categorical(\"max_length\", [128, 160, 192])\n",
    "    num_epochs    = trial.suggest_int(\"num_epochs\", 2, 4)\n",
    "    warmup_ratio  = trial.suggest_float(\"warmup_ratio\", 0.05, 0.2)\n",
    "    use_lora      = trial.suggest_categorical(\"use_lora\", [False, True])\n",
    "    lora_r        = trial.suggest_categorical(\"lora_r\", [8, 16]) if use_lora else 0\n",
    "    lora_alpha    = trial.suggest_categorical(\"lora_alpha\", [16, 32]) if use_lora else 0\n",
    "    lora_dropout  = trial.suggest_float(\"lora_dropout\", 0.0, 0.15) if use_lora else 0.0\n",
    "\n",
    "    wandb_run = wandb.init(project=\"tweets-encoder-deberta-raw\", config=trial.params)\n",
    "\n",
    "    lora_cfg = None\n",
    "    if use_lora:\n",
    "        lora_cfg = LoraConfig(\n",
    "            r=lora_r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n",
    "            target_modules=[\"query_proj\",\"key_proj\",\"value_proj\",\"output_proj\"],  # DeBERTa naming can vary; fallback to all linear layers if needed\n",
    "            bias=\"none\", task_type=\"SEQ_CLS\"\n",
    "        )\n",
    "\n",
    "    model = build_deberta_classif(num_labels=len(LABELS), lora_cfg=lora_cfg)\n",
    "\n",
    "    tr_ds = EncTweetSet(train_df, enc_tokenizer, max_len=max_len)\n",
    "    va_ds = EncTweetSet(val_df,   enc_tokenizer, max_len=max_len)\n",
    "    tr_ld = DataLoader(tr_ds, batch_size=batch_size, shuffle=True,  collate_fn=enc_pad_collate)\n",
    "    va_ld = DataLoader(va_ds, batch_size=batch_size, shuffle=False, collate_fn=enc_pad_collate)\n",
    "\n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    steps = num_epochs * len(tr_ld)\n",
    "    warmup = int(warmup_ratio * steps)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, warmup, steps)\n",
    "\n",
    "    best_f1, no_improve = -1.0, 0\n",
    "    for ep in range(1, num_epochs+1):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for batch in tr_ld:\n",
    "            for k in batch: batch[k] = batch[k].to(model.device)\n",
    "            out = model(**batch)\n",
    "            loss = out.loss\n",
    "            optimizer.zero_grad(); loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step(); scheduler.step()\n",
    "            total += loss.item()*batch[\"input_ids\"].size(0)\n",
    "        tr_loss = total/len(tr_ld.dataset)\n",
    "        va_f1 = enc_eval_macro_f1(model, va_ld)\n",
    "        wandb.log({\"epoch\": ep, \"train_loss\": tr_loss, \"val_macro_f1\": va_f1})\n",
    "        trial.report(va_f1, ep)\n",
    "        if trial.should_prune(): raise optuna.TrialPruned()\n",
    "        if va_f1 > best_f1:\n",
    "            best_f1, no_improve = va_f1, 0\n",
    "            model.save_pretrained(f\"best_deberta_raw_trial_{trial.number}\")\n",
    "            enc_tokenizer.save_pretrained(f\"best_deberta_raw_trial_{trial.number}\")\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                break\n",
    "\n",
    "    wandb.summary[\"best_val_macro_f1\"] = best_f1\n",
    "    wandb_run.finish()\n",
    "    return best_f1\n",
    "\n",
    "study_enc_raw = optuna.create_study(direction=\"maximize\")\n",
    "study_enc_raw.optimize(objective_enc_raw, n_trials=2)\n",
    "print(\"Best RAW DeBERTa macro-F1:\", study_enc_raw.best_value, study_enc_raw.best_trial.params)\n",
    "\n",
    "# ---------- HF Trainer + Optuna for DeBERTa ----------\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "def build_enc_trainer(params):\n",
    "    max_len = params[\"max_length\"]\n",
    "    def tok_fn(ex):\n",
    "        out = enc_tokenizer(ex[\"clean_text\"], truncation=True, max_length=max_len)\n",
    "        out[\"labels\"] = LABEL2ID[ex[\"Sentiment\"]]\n",
    "        return out\n",
    "    dtr = Dataset.from_pandas(train_df[[\"clean_text\",\"Sentiment\"]]).map(tok_fn)\n",
    "    dva = Dataset.from_pandas(val_df[[\"clean_text\",\"Sentiment\"]]).map(tok_fn)\n",
    "\n",
    "    lora_cfg = None\n",
    "    if params[\"use_lora\"]:\n",
    "        lora_cfg = LoraConfig(\n",
    "            r=params[\"lora_r\"], lora_alpha=params[\"lora_alpha\"], lora_dropout=params[\"lora_dropout\"],\n",
    "            target_modules=[\"query_proj\",\"key_proj\",\"value_proj\",\"output_proj\"], bias=\"none\", task_type=\"SEQ_CLS\"\n",
    "        )\n",
    "\n",
    "    model = build_deberta_classif(num_labels=len(LABELS), lora_cfg=lora_cfg)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"./hf_deberta_out\",\n",
    "        per_device_train_batch_size=params[\"batch_size\"],\n",
    "        per_device_eval_batch_size=params[\"batch_size\"],\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        weight_decay=params[\"weight_decay\"],\n",
    "        num_train_epochs=params[\"num_epochs\"],\n",
    "        warmup_ratio=params[\"warmup_ratio\"],\n",
    "        logging_strategy=\"steps\", logging_steps=50,\n",
    "        evaluation_strategy=\"steps\", eval_steps=200,\n",
    "        save_strategy=\"steps\", save_steps=200,\n",
    "        load_best_model_at_end=True, metric_for_best_model=\"eval_f1\",\n",
    "        report_to=[\"wandb\"], fp16=torch.cuda.is_available()\n",
    "    )\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = logits.argmax(axis=-1)\n",
    "        return {\n",
    "            \"accuracy\": (preds == labels).mean().item() if hasattr((preds == labels).mean(), \"item\") else float((preds == labels).mean()),\n",
    "            \"f1_macro\": f1_score(labels, preds, average=\"macro\")\n",
    "        }\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model, args=args,\n",
    "        train_dataset=dtr, eval_dataset=dva,\n",
    "        data_collator=DataCollatorWithPadding(enc_tokenizer),\n",
    "        tokenizer=enc_tokenizer, compute_metrics=compute_metrics\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "def objective_enc_hf(trial: optuna.Trial):\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 5e-4),\n",
    "        \"weight_decay\":  trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-4),\n",
    "        \"batch_size\":    trial.suggest_categorical(\"batch_size\", [16, 32, 64]),\n",
    "        \"max_length\":    trial.suggest_categorical(\"max_length\", [128, 160, 192]),\n",
    "        \"num_epochs\":    trial.suggest_int(\"num_epochs\", 2, 4),\n",
    "        \"warmup_ratio\":  trial.suggest_float(\"warmup_ratio\", 0.05, 0.2),\n",
    "        \"use_lora\":      trial.suggest_categorical(\"use_lora\", [False, True]),\n",
    "        \"lora_r\":        trial.suggest_categorical(\"lora_r\", [8, 16]) if trial.params.get(\"use_lora\", False) else 0,\n",
    "        \"lora_alpha\":    trial.suggest_categorical(\"lora_alpha\", [16, 32]) if trial.params.get(\"use_lora\", False) else 0,\n",
    "        \"lora_dropout\":  trial.suggest_float(\"lora_dropout\", 0.0, 0.15) if trial.params.get(\"use_lora\", False) else 0.0,\n",
    "    }\n",
    "    run = wandb.init(project=\"tweets-encoder-deberta-trainer\", config=params)\n",
    "    trainer = build_enc_trainer(params)\n",
    "    trainer.train()\n",
    "\n",
    "    # return macro-F1 from best checkpoint on val set\n",
    "    metrics = trainer.evaluate()\n",
    "    val_f1 = metrics.get(\"eval_f1_macro\", metrics.get(\"eval_f1\", 0.0))\n",
    "    wandb.summary[\"val_macro_f1\"] = val_f1\n",
    "\n",
    "    out_dir = f\"best_deberta_trainer_trial_{trial.number}\"\n",
    "    trainer.save_model(out_dir); enc_tokenizer.save_pretrained(out_dir)\n",
    "    run.finish()\n",
    "    return val_f1\n",
    "\n",
    "study_enc_hf = optuna.create_study(direction=\"maximize\")\n",
    "study_enc_hf.optimize(objective_enc_hf, n_trials=2)\n",
    "print(\"Best HF DeBERTa macro-F1:\", study_enc_hf.best_value, study_enc_hf.best_trial.params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
