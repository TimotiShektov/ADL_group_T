{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46845c9f-bcb1-4484-90d1-49208995f7e9",
   "metadata": {},
   "source": [
    "<h1 style='text-align: center;'> Part B - Tweets Fine-tuning, model training and compression with comparison </h1>\n",
    "<h3 style='text-align: center;'> Group T, IDs: 316398387 ,318481447</h3>\n",
    "\n",
    "Based on the preprocessing phase done in the previous section, we will approach this with 2 models, one is **Encoder(only) based** model, and the other will be **Decoder only**\n",
    "<h6 style='text-align: left;'>similar imports like previous part:</h6>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b9fb07fe-98df-4df0-b99d-b53bc8a5155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig, AutoTokenizer, get_linear_schedule_with_warmup, AutoModelForSequenceClassification\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import seaborn as sns\n",
    "from decouple import Config, RepositoryEnv \n",
    "import emoji\n",
    "import optuna\n",
    "from langdetect import detect, DetectorFactory\n",
    "from ftfy import fix_text\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import wandb\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import re\n",
    "import warnings\n",
    "import optuna\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "wandb.login(key=Config(RepositoryEnv(\"./.env\")).get('wandb_api_key'));  # If not necessary comment this line of W&B, or change to wandb.login() and pase API key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1c2cf426-1cb5-4fc1-ba88-65e70b0cecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"preprocessed_tweets.csv\"); df.head(); # The preprocessed tweets csv should be in same root folder after cloning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b8544b68-ed6c-4af2-babe-f24f334204b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28609, 2), (7356, 2), (4905, 2))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLASS_TEXTS = ['extremely negative','negative','neutral','positive','extremely positive']  # order matters\n",
    "LABEL2ID = {s:i for i,s in enumerate(CLASS_TEXTS)}\n",
    "ID2LABEL = {i:s for s,i in LABEL2ID.items()}\n",
    "\n",
    "train_df, tmp = train_test_split(df, test_size=0.3, stratify=df['Sentiment'], random_state=42)\n",
    "val_df, test_df = train_test_split(tmp, test_size=0.4, stratify=tmp['Sentiment'], random_state=42)\n",
    "train_df.shape, val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a136797-2077-475a-99fb-35ec03fad6a9",
   "metadata": {},
   "source": [
    "### Decoder-only: TinlyLLAMA model\n",
    "\n",
    "We will start with the decoder only model where our samples need to be in a format of instruction. \n",
    "we will train the decoder in a generative way and its format will be like: \n",
    "\"Tweet: {text}\\nSentiment: {label_text}\" and we will mask the prompt part.\n",
    "<br>\n",
    "The **Raw Pytorch fine-tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "73ad5818-4946-41ee-9630-9101daa8a72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # swap to Mistral/Llama-3 if you have VRAM\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "tokenizer.padding_side = \"right\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def build_prompt(text: str) -> str:\n",
    "    return f\"Tweet: {text}\\nSentiment:\"\n",
    "\n",
    "def build_target(label_text: str) -> str:\n",
    "    return label_text + tokenizer.eos_token\n",
    "\n",
    "class GenTweetSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=160):\n",
    "        self.prompts = [build_prompt(t) for t in df[\"clean_text\"]]\n",
    "        self.targets = [build_target(s) for s in df[\"Sentiment\"]]\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self): return len(self.prompts)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        s_enc = self.tok(self.prompts[i], add_special_tokens=False)\n",
    "        t_enc = self.tok(self.targets[i], add_special_tokens=False)\n",
    "        input_ids  = s_enc[\"input_ids\"] + t_enc[\"input_ids\"]\n",
    "        attn_mask  = [1]*len(input_ids)\n",
    "        labels     = [-100]*len(s_enc[\"input_ids\"]) + t_enc[\"input_ids\"]\n",
    "        if len(input_ids) > self.max_len:\n",
    "            cut = self.max_len\n",
    "            # keep at least target portion\n",
    "            keep_src = max(0, cut - len(t_enc[\"input_ids\"]))\n",
    "            input_ids = input_ids[:keep_src] + t_enc[\"input_ids\"][:cut-keep_src]\n",
    "            attn_mask = [1]*len(input_ids)\n",
    "            labels    = [-100]*keep_src + t_enc[\"input_ids\"][:cut-keep_src]\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attn_mask, \"labels\": labels}\n",
    "\n",
    "def pad_collate(batch):\n",
    "    maxlen = max(len(x[\"input_ids\"]) for x in batch)\n",
    "    pad_id = tok.pad_token_id\n",
    "    out = {\"input_ids\":[], \"attention_mask\":[], \"labels\":[]}\n",
    "    for x in batch:\n",
    "        pad = maxlen - len(x[\"input_ids\"])\n",
    "        out[\"input_ids\"].append(x[\"input_ids\"] + [pad_id]*pad)\n",
    "        out[\"attention_mask\"].append(x[\"attention_mask\"] + [0]*pad)\n",
    "        out[\"labels\"].append(x[\"labels\"] + [-100]*pad)\n",
    "    return {k: torch.tensor(v) for k,v in out.items()}\n",
    "\n",
    "def build_causal_lora(lora_r=16, lora_alpha=32, lora_dropout=0.05):\n",
    "    quant = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True,\n",
    "                               bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME, quantization_config=quant, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    "    )\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    peft = LoraConfig(\n",
    "        r=lora_r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    model = get_peft_model(model, peft)\n",
    "    return model\n",
    "\n",
    "def extract_label(gen_text: str):\n",
    "    s = gen_text.lower()\n",
    "    for lab in CLASS_TEXTS:\n",
    "        if lab.lower() in s:\n",
    "            return lab\n",
    "    # fallback heuristics\n",
    "    if \"very positive\" in s or \"extremely positive\" in s: return \"Extremely Positive\"\n",
    "    if \"very negative\" in s or \"extremely negative\" in s: return \"Extremely Negative\"\n",
    "    if \"positive\" in s: return \"Positive\"\n",
    "    if \"negative\" in s: return \"Negative\"\n",
    "    if \"neutral\" in s:  return \"Neutral\"\n",
    "    return \"Neutral\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_macro_f1(model, df_eval, max_len=160, max_new_tokens=4, bs=64):\n",
    "    prompts = [build_prompt(t) for t in df_eval[\"clean_text\"]]\n",
    "    gold    = df_eval[\"Sentiment\"].tolist()\n",
    "    preds = []\n",
    "    for i in range(0, len(prompts), bs):\n",
    "        batch = prompts[i:i+bs]\n",
    "        enc = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len).to(model.device)\n",
    "        out = model.generate(\n",
    "            **enc, max_new_tokens=max_new_tokens, do_sample=False, eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        input_lens = enc[\"attention_mask\"].sum(dim=1).tolist()\n",
    "        for j, ids in enumerate(out):\n",
    "            gen_ids = ids[input_lens[j]:]\n",
    "            text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "            preds.append(extract_label(text))\n",
    "    y_true = [LABEL2ID[x] for x in gold]\n",
    "    y_pred = [LABEL2ID.get(x, LABEL2ID[\"Neutral\"]) for x in preds]\n",
    "    return f1_score(y_true, y_pred, average=\"macro\")\n",
    "  \n",
    "\n",
    "def run_epoch(model, loader, optimizer=None, scheduler=None, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "    total_loss = 0.0\n",
    "    for batch in loader:\n",
    "        for k in batch: batch[k] = batch[k].to(model.device)\n",
    "        with torch.set_grad_enabled(train):\n",
    "            out = model(**batch)\n",
    "            loss = out.loss\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                if scheduler: scheduler.step()\n",
    "        total_loss += loss.item() * batch[\"input_ids\"].size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def objective_raw(trial: optuna.Trial):\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
    "    weight_decay  = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-4)\n",
    "    patience      = trial.suggest_int(\"patience\", 7, 10)\n",
    "    batch_size    = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    num_layers    = trial.suggest_int(\"num_layers\", 1, 3)  # placeholder\n",
    "    max_len       = trial.suggest_categorical(\"max_length\", [128, 160, 192])\n",
    "    num_epochs    = trial.suggest_int(\"num_epochs\", 2, 4)\n",
    "    warmup_ratio  = trial.suggest_float(\"warmup_ratio\", 0.05, 0.2)\n",
    "    lora_r        = trial.suggest_categorical(\"lora_r\", [8, 16, 32])\n",
    "    lora_alpha    = trial.suggest_categorical(\"lora_alpha\", [16, 32, 64])\n",
    "    lora_dropout  = trial.suggest_float(\"lora_dropout\", 0.0, 0.2)\n",
    "\n",
    "    wandb_run = wandb.init(project=\"tweets-decoder-gentask-raw\", config=trial.params)\n",
    "\n",
    "    model = build_causal_lora(lora_r, lora_alpha, lora_dropout)\n",
    "\n",
    "    tr_ds = GenTweetSet(train_df, tokenizer, max_len=max_len)\n",
    "    va_ds = GenTweetSet(val_df, tokenizer, max_len=max_len)\n",
    "    tr_ld = DataLoader(tr_ds, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
    "    va_ld = DataLoader(va_ds, batch_size=batch_size, shuffle=False, collate_fn=pad_collate)\n",
    "\n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    steps = num_epochs * len(tr_ld)\n",
    "    warmup = int(warmup_ratio * steps)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, warmup, steps)\n",
    "\n",
    "    best_f1, no_improve = -1.0, 0\n",
    "    for ep in range(1, num_epochs+1):\n",
    "        tr_loss = run_epoch(model, tr_ld, optimizer, scheduler, train=True)\n",
    "        va_loss = run_epoch(model, va_ld, train=False)\n",
    "        va_f1 = eval_macro_f1(model, val_df, max_len=max_len)\n",
    "        wandb.log({\"epoch\": ep, \"train_loss\": tr_loss, \"val_loss\": va_loss, \"val_macro_f1\": va_f1})\n",
    "        trial.report(va_f1, ep)\n",
    "        if trial.should_prune(): raise optuna.TrialPruned()\n",
    "        if va_f1 > best_f1:\n",
    "            best_f1, no_improve = va_f1, 0\n",
    "            model.save_pretrained(f\"best_raw_gen_trial_{trial.number}\")\n",
    "            tokenizer.save_pretrained(f\"best_raw_gen_trial_{trial.number}\")\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                break\n",
    "\n",
    "    wandb.summary[\"best_val_macro_f1\"] = best_f1\n",
    "    wandb_run.finish()\n",
    "    return best_f1\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective_raw, n_trials=2)\n",
    "print(\"Best raw macro-F1:\", study.best_value, study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6bbda0-67e4-4a8a-8d92-8e1364936ad7",
   "metadata": {},
   "source": [
    "**The HF trainer** + optuna and W&B experiment like the above raw training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bfe67c00-34bd-4b0f-ba62-9ef4ac211a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def hf_build_decoder_ds(df, max_len=160):\n",
    "    # map into prompt -> labels (causal LM): mask source tokens with -100\n",
    "    def _enc(ex):\n",
    "        src = build_prompt(ex[\"clean_text\"])\n",
    "        tgt = build_target(ex[\"Sentiment\"])\n",
    "        s = tokenizer(src, add_special_tokens=False)\n",
    "        t = tokenizer(tgt, add_special_tokens=False)\n",
    "        inp = s[\"input_ids\"] + t[\"input_ids\"]\n",
    "        att = [1]*len(inp)\n",
    "        lab = [-100]*len(s[\"input_ids\"]) + t[\"input_ids\"]\n",
    "        # truncate keeping all target tokens\n",
    "        if len(inp) > max_len:\n",
    "            keep_src = max(0, max_len - len(t[\"input_ids\"]))\n",
    "            inp = inp[:keep_src] + t[\"input_ids\"][:max_len-keep_src]\n",
    "            att = [1]*len(inp)\n",
    "            lab = [-100]*keep_src + t[\"input_ids\"][:max_len-keep_src]\n",
    "        return {\"input_ids\": inp, \"attention_mask\": att, \"labels\": lab}\n",
    "\n",
    "    cols = [\"input_ids\",\"attention_mask\",\"labels\"]\n",
    "    ds = Dataset.from_pandas(df[[\"clean_text\",\"Sentiment\"]])\n",
    "    ds = ds.map(_enc, remove_columns=[c for c in ds.column_names if c not in [\"clean_text\",\"Sentiment\"]])\n",
    "    ds = ds.remove_columns([\"clean_text\",\"Sentiment\"])\n",
    "    return ds\n",
    "\n",
    "def hf_decoder_collator(features):\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    maxlen = max(len(f[\"input_ids\"]) for f in features)\n",
    "    batch = {\"input_ids\":[], \"attention_mask\":[], \"labels\":[]}\n",
    "    for f in features:\n",
    "        pad = maxlen - len(f[\"input_ids\"])\n",
    "        batch[\"input_ids\"].append(f[\"input_ids\"] + [pad_id]*pad)\n",
    "        batch[\"attention_mask\"].append(f[\"attention_mask\"] + [0]*pad)\n",
    "        batch[\"labels\"].append(f[\"labels\"] + [-100]*pad)\n",
    "    return {k: torch.tensor(v) for k,v in batch.items()}\n",
    "\n",
    "# ---------- Trainer factory ----------\n",
    "\n",
    "def build_decoder_trainer(params):\n",
    "    max_len = params[\"max_length\"]\n",
    "    d_train = hf_build_decoder_ds(train_df, max_len=max_len)\n",
    "    d_val   = hf_build_decoder_ds(val_df,   max_len=max_len)\n",
    "\n",
    "    quant = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME, quantization_config=quant, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    "    )\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    peft_cfg = LoraConfig(\n",
    "        r=params[\"lora_r\"], lora_alpha=params[\"lora_alpha\"], lora_dropout=params[\"lora_dropout\"],\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    model = get_peft_model(model, peft_cfg)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"./hf_decoder_out\",\n",
    "        per_device_train_batch_size=params[\"batch_size\"],\n",
    "        per_device_eval_batch_size=params[\"batch_size\"],\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        weight_decay=params[\"weight_decay\"],\n",
    "        num_train_epochs=params[\"num_epochs\"],\n",
    "        warmup_ratio=params[\"warmup_ratio\"],\n",
    "        logging_strategy=\"steps\", logging_steps=50,\n",
    "        evaluation_strategy=\"steps\", eval_steps=200,\n",
    "        save_strategy=\"steps\", save_steps=200,\n",
    "        load_best_model_at_end=True, metric_for_best_model=\"eval_loss\",\n",
    "        gradient_checkpointing=True,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=[\"wandb\"]\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model, args=args,\n",
    "        train_dataset=d_train, eval_dataset=d_val,\n",
    "        data_collator=hf_decoder_collator, tokenizer=tokenizer\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "# ---------- Optuna objective for HF Trainer ----------\n",
    "\n",
    "def objective_hf_decoder(trial: optuna.Trial):\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3),\n",
    "        \"weight_decay\":  trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-4),\n",
    "        \"patience\":      trial.suggest_int(\"patience\", 7, 10),   # used only for raw; kept for parity\n",
    "        \"batch_size\":    trial.suggest_categorical(\"batch_size\", [32, 64, 128]),\n",
    "        \"num_layers\":    trial.suggest_int(\"num_layers\", 1, 3),  # placeholder (not used directly)\n",
    "        \"max_length\":    trial.suggest_categorical(\"max_length\", [128, 160, 192]),\n",
    "        \"num_epochs\":    trial.suggest_int(\"num_epochs\", 2, 4),\n",
    "        \"warmup_ratio\":  trial.suggest_float(\"warmup_ratio\", 0.05, 0.2),\n",
    "        \"lora_r\":        trial.suggest_categorical(\"lora_r\", [8, 16, 32]),\n",
    "        \"lora_alpha\":    trial.suggest_categorical(\"lora_alpha\", [16, 32, 64]),\n",
    "        \"lora_dropout\":  trial.suggest_float(\"lora_dropout\", 0.0, 0.2),\n",
    "    }\n",
    "\n",
    "    run = wandb.init(project=\"tweets-decoder-gentask-trainer\", config=params)\n",
    "    trainer = build_decoder_trainer(params)\n",
    "    trainer.train()\n",
    "\n",
    "    # compute macro-F1 via generation (same helper you already have)\n",
    "    val_f1 = eval_macro_f1(trainer.model, val_df, max_len=params[\"max_length\"])\n",
    "    wandb.summary[\"val_macro_f1_gen\"] = val_f1\n",
    "\n",
    "    # persist best\n",
    "    out_dir = f\"best_trainer_gen_trial_{trial.number}\"\n",
    "    trainer.save_model(out_dir); tokenizer.save_pretrained(out_dir)\n",
    "    run.finish()\n",
    "    return val_f1\n",
    "\n",
    "# Example separate study for HF:\n",
    "study_hf = optuna.create_study(direction=\"maximize\")\n",
    "study_hf.optimize(objective_hf_decoder, n_trials=2)\n",
    "print(\"Best HF (decoder) macro-F1:\", study_hf.best_value, study_hf.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5580bd0e-98d7-468a-9194-06a29952e09c",
   "metadata": {},
   "source": [
    "### Encoder-Only: DeBERTa-v3 based classification: \n",
    "\n",
    "In this part we will apply same process as in the decoder part but now on the encoder part so we can see what is more robust and performs better way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2f949e69-8d6e-47af-b85f-0861b1885ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Encoder (DeBERTa-v3) setup -----\n",
    "ENC_MODEL = \"microsoft/deberta-v3-base\"\n",
    "enc_tokenizer = AutoTokenizer.from_pretrained(ENC_MODEL, use_fast=True)\n",
    "\n",
    "LABELS = CLASS_TEXTS  # [\"Extremely Negative\",\"Negative\",\"Neutral\",\"Positive\",\"Extremely Positive\"]\n",
    "ID2LABEL = {i:lab for i,lab in enumerate(LABELS)}\n",
    "LABEL2ID = {lab:i for i,lab in enumerate(LABELS)}\n",
    "\n",
    "class EncTweetSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=160):\n",
    "        self.texts = df[\"clean_text\"].tolist()\n",
    "        self.labels = [LABEL2ID[s] for s in df[\"Sentiment\"].tolist()]\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, i):\n",
    "        enc = self.tok(self.texts[i], truncation=True, max_length=self.max_len)\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"],\n",
    "            \"attention_mask\": enc[\"attention_mask\"],\n",
    "            \"labels\": self.labels[i]\n",
    "        }\n",
    "\n",
    "def enc_pad_collate(batch):\n",
    "    pad_id = enc_tokenizer.pad_token_id\n",
    "    maxlen = max(len(x[\"input_ids\"]) for x in batch)\n",
    "    out = {\"input_ids\":[], \"attention_mask\":[], \"labels\":[]}\n",
    "    for x in batch:\n",
    "        pad = maxlen - len(x[\"input_ids\"])\n",
    "        out[\"input_ids\"].append(x[\"input_ids\"] + [pad_id]*pad)\n",
    "        out[\"attention_mask\"].append(x[\"attention_mask\"] + [0]*pad)\n",
    "        out[\"labels\"].append(x[\"labels\"])\n",
    "    return {k: torch.tensor(v) for k,v in out.items()}\n",
    "\n",
    "def build_deberta_classif(num_labels=5, lora_cfg=None):\n",
    "    quant = BitsAndBytesConfig(load_in_8bit=True)  # 8-bit encoder is usually fine; or fp16 if you prefer\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        ENC_MODEL, num_labels=num_labels, id2label=ID2LABEL, label2id=LABEL2ID,\n",
    "        quantization_config=quant, device_map=\"auto\"\n",
    "    )\n",
    "    if lora_cfg is not None:\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        model = get_peft_model(model, lora_cfg)\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def enc_eval_macro_f1(model, loader):\n",
    "    model.eval()\n",
    "    preds, gold = [], []\n",
    "    for batch in loader:\n",
    "        for k in batch: batch[k] = batch[k].to(model.device)\n",
    "        out = model(**batch)\n",
    "        p = out.logits.argmax(dim=-1).detach().cpu().tolist()\n",
    "        y = batch[\"labels\"].detach().cpu().tolist()\n",
    "        preds.extend(p); gold.extend(y)\n",
    "    return f1_score(gold, preds, average=\"macro\")\n",
    "\n",
    "# ---------- RAW PyTorch + Optuna for DeBERTa ----------\n",
    "\n",
    "def objective_enc_raw(trial: optuna.Trial):\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 5e-4)\n",
    "    weight_decay  = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-4)\n",
    "    patience      = trial.suggest_int(\"patience\", 5, 10)\n",
    "    batch_size    = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "    max_len       = trial.suggest_categorical(\"max_length\", [128, 160, 192])\n",
    "    num_epochs    = trial.suggest_int(\"num_epochs\", 2, 4)\n",
    "    warmup_ratio  = trial.suggest_float(\"warmup_ratio\", 0.05, 0.2)\n",
    "    use_lora      = trial.suggest_categorical(\"use_lora\", [False, True])\n",
    "    lora_r        = trial.suggest_categorical(\"lora_r\", [8, 16]) if use_lora else 0\n",
    "    lora_alpha    = trial.suggest_categorical(\"lora_alpha\", [16, 32]) if use_lora else 0\n",
    "    lora_dropout  = trial.suggest_float(\"lora_dropout\", 0.0, 0.15) if use_lora else 0.0\n",
    "\n",
    "    wandb_run = wandb.init(project=\"tweets-encoder-deberta-raw\", config=trial.params)\n",
    "\n",
    "    lora_cfg = None\n",
    "    if use_lora:\n",
    "        lora_cfg = LoraConfig(\n",
    "            r=lora_r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n",
    "            target_modules=[\"query_proj\",\"key_proj\",\"value_proj\",\"output_proj\"],  # DeBERTa naming can vary; fallback to all linear layers if needed\n",
    "            bias=\"none\", task_type=\"SEQ_CLS\"\n",
    "        )\n",
    "\n",
    "    model = build_deberta_classif(num_labels=len(LABELS), lora_cfg=lora_cfg)\n",
    "\n",
    "    tr_ds = EncTweetSet(train_df, enc_tokenizer, max_len=max_len)\n",
    "    va_ds = EncTweetSet(val_df,   enc_tokenizer, max_len=max_len)\n",
    "    tr_ld = DataLoader(tr_ds, batch_size=batch_size, shuffle=True,  collate_fn=enc_pad_collate)\n",
    "    va_ld = DataLoader(va_ds, batch_size=batch_size, shuffle=False, collate_fn=enc_pad_collate)\n",
    "\n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    steps = num_epochs * len(tr_ld)\n",
    "    warmup = int(warmup_ratio * steps)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, warmup, steps)\n",
    "\n",
    "    best_f1, no_improve = -1.0, 0\n",
    "    for ep in range(1, num_epochs+1):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for batch in tr_ld:\n",
    "            for k in batch: batch[k] = batch[k].to(model.device)\n",
    "            out = model(**batch)\n",
    "            loss = out.loss\n",
    "            optimizer.zero_grad(); loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step(); scheduler.step()\n",
    "            total += loss.item()*batch[\"input_ids\"].size(0)\n",
    "        tr_loss = total/len(tr_ld.dataset)\n",
    "        va_f1 = enc_eval_macro_f1(model, va_ld)\n",
    "        wandb.log({\"epoch\": ep, \"train_loss\": tr_loss, \"val_macro_f1\": va_f1})\n",
    "        trial.report(va_f1, ep)\n",
    "        if trial.should_prune(): raise optuna.TrialPruned()\n",
    "        if va_f1 > best_f1:\n",
    "            best_f1, no_improve = va_f1, 0\n",
    "            model.save_pretrained(f\"best_deberta_raw_trial_{trial.number}\")\n",
    "            enc_tokenizer.save_pretrained(f\"best_deberta_raw_trial_{trial.number}\")\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                break\n",
    "\n",
    "    wandb.summary[\"best_val_macro_f1\"] = best_f1\n",
    "    wandb_run.finish()\n",
    "    return best_f1\n",
    "\n",
    "study_enc_raw = optuna.create_study(direction=\"maximize\")\n",
    "study_enc_raw.optimize(objective_enc_raw, n_trials=2)\n",
    "print(\"Best RAW DeBERTa macro-F1:\", study_enc_raw.best_value, study_enc_raw.best_trial.params)\n",
    "\n",
    "# ---------- HF Trainer + Optuna for DeBERTa ----------\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "def build_enc_trainer(params):\n",
    "    max_len = params[\"max_length\"]\n",
    "    def tok_fn(ex):\n",
    "        out = enc_tokenizer(ex[\"clean_text\"], truncation=True, max_length=max_len)\n",
    "        out[\"labels\"] = LABEL2ID[ex[\"Sentiment\"]]\n",
    "        return out\n",
    "    dtr = Dataset.from_pandas(train_df[[\"clean_text\",\"Sentiment\"]]).map(tok_fn)\n",
    "    dva = Dataset.from_pandas(val_df[[\"clean_text\",\"Sentiment\"]]).map(tok_fn)\n",
    "\n",
    "    lora_cfg = None\n",
    "    if params[\"use_lora\"]:\n",
    "        lora_cfg = LoraConfig(\n",
    "            r=params[\"lora_r\"], lora_alpha=params[\"lora_alpha\"], lora_dropout=params[\"lora_dropout\"],\n",
    "            target_modules=[\"query_proj\",\"key_proj\",\"value_proj\",\"output_proj\"], bias=\"none\", task_type=\"SEQ_CLS\"\n",
    "        )\n",
    "\n",
    "    model = build_deberta_classif(num_labels=len(LABELS), lora_cfg=lora_cfg)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"./hf_deberta_out\",\n",
    "        per_device_train_batch_size=params[\"batch_size\"],\n",
    "        per_device_eval_batch_size=params[\"batch_size\"],\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        weight_decay=params[\"weight_decay\"],\n",
    "        num_train_epochs=params[\"num_epochs\"],\n",
    "        warmup_ratio=params[\"warmup_ratio\"],\n",
    "        logging_strategy=\"steps\", logging_steps=50,\n",
    "        evaluation_strategy=\"steps\", eval_steps=200,\n",
    "        save_strategy=\"steps\", save_steps=200,\n",
    "        load_best_model_at_end=True, metric_for_best_model=\"eval_f1\",\n",
    "        report_to=[\"wandb\"], fp16=torch.cuda.is_available()\n",
    "    )\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = logits.argmax(axis=-1)\n",
    "        return {\n",
    "            \"accuracy\": (preds == labels).mean().item() if hasattr((preds == labels).mean(), \"item\") else float((preds == labels).mean()),\n",
    "            \"f1_macro\": f1_score(labels, preds, average=\"macro\")\n",
    "        }\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model, args=args,\n",
    "        train_dataset=dtr, eval_dataset=dva,\n",
    "        data_collator=DataCollatorWithPadding(enc_tokenizer),\n",
    "        tokenizer=enc_tokenizer, compute_metrics=compute_metrics\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "def objective_enc_hf(trial: optuna.Trial):\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 5e-4),\n",
    "        \"weight_decay\":  trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-4),\n",
    "        \"batch_size\":    trial.suggest_categorical(\"batch_size\", [16, 32, 64]),\n",
    "        \"max_length\":    trial.suggest_categorical(\"max_length\", [128, 160, 192]),\n",
    "        \"num_epochs\":    trial.suggest_int(\"num_epochs\", 2, 4),\n",
    "        \"warmup_ratio\":  trial.suggest_float(\"warmup_ratio\", 0.05, 0.2),\n",
    "        \"use_lora\":      trial.suggest_categorical(\"use_lora\", [False, True]),\n",
    "        \"lora_r\":        trial.suggest_categorical(\"lora_r\", [8, 16]) if trial.params.get(\"use_lora\", False) else 0,\n",
    "        \"lora_alpha\":    trial.suggest_categorical(\"lora_alpha\", [16, 32]) if trial.params.get(\"use_lora\", False) else 0,\n",
    "        \"lora_dropout\":  trial.suggest_float(\"lora_dropout\", 0.0, 0.15) if trial.params.get(\"use_lora\", False) else 0.0,\n",
    "    }\n",
    "    run = wandb.init(project=\"tweets-encoder-deberta-trainer\", config=params)\n",
    "    trainer = build_enc_trainer(params)\n",
    "    trainer.train()\n",
    "\n",
    "    # return macro-F1 from best checkpoint on val set\n",
    "    metrics = trainer.evaluate()\n",
    "    val_f1 = metrics.get(\"eval_f1_macro\", metrics.get(\"eval_f1\", 0.0))\n",
    "    wandb.summary[\"val_macro_f1\"] = val_f1\n",
    "\n",
    "    out_dir = f\"best_deberta_trainer_trial_{trial.number}\"\n",
    "    trainer.save_model(out_dir); enc_tokenizer.save_pretrained(out_dir)\n",
    "    run.finish()\n",
    "    return val_f1\n",
    "\n",
    "study_enc_hf = optuna.create_study(direction=\"maximize\")\n",
    "study_enc_hf.optimize(objective_enc_hf, n_trials=2)\n",
    "print(\"Best HF DeBERTa macro-F1:\", study_enc_hf.best_value, study_enc_hf.best_trial.params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01ad29309aa346e2b6e4b9233ebb2118": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "01c7dc7dca27491ba4315951d536ddc5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "02fb76c9a50e4eeb8086eff348015a4d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_3d141a0bf1ed4477bcf421657a2e4ba8",
        "IPY_MODEL_06bc72330f60466fad99d785b3f3ba5d"
       ],
       "layout": "IPY_MODEL_04b4fdf91d4d4f64a79bfa481e7a36a8"
      }
     },
     "03d29e2aa3014130b8294a271bf77269": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_b60da126220448fc9ccdfb427535fafc",
       "style": "IPY_MODEL_4a7d4c2fcb164fe588c3d3ec1e25f283"
      }
     },
     "047a1186f5924f139e38591683e807f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "04b4fdf91d4d4f64a79bfa481e7a36a8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "05bc6c6e43474c22b1e0d05919b388fa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "06bc72330f60466fad99d785b3f3ba5d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_0bc1ba2d3ae54f4e969d8f69a1cda363",
       "max": 1,
       "style": "IPY_MODEL_1a003478101f4471b5ffd824ae9ae03e"
      }
     },
     "09d33b5069b04db99bb39edd0695a43e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_291513d41df7493bb1df4d2137fd6386",
        "IPY_MODEL_50e9636aab624f0c84c552eb83a61f01"
       ],
       "layout": "IPY_MODEL_c8c83f22ab014ebab54febc4aa8e40e6"
      }
     },
     "0a54c1e1104a48d1a44bb6213ee97686": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0b5846d07b0a40f09ae7b7807fe2f8b8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0bc1ba2d3ae54f4e969d8f69a1cda363": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0d1f9adb08d24e93a1554eff2c899535": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_03d29e2aa3014130b8294a271bf77269",
        "IPY_MODEL_2ed5a880a81948e6a315f84b3f1a7b92"
       ],
       "layout": "IPY_MODEL_58cbaf7a57e54c06b5305ab9cc385884"
      }
     },
     "0ed1495265b94344b629fdd762854463": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0f1b6fec64ec49ff94838910dca2ec0b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "20px"
      }
     },
     "12109f7556df4c999f08c3c360a9e001": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "1532dbdff0a14110bea2e49b3c208969": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_de645524dde34a9b8f69592db8338c36",
       "style": "IPY_MODEL_fa9d6dbe395f49b88e4ca7814aa28d0d",
       "value": " 608/608 [00:00&lt;00:00, 62.3kB/s]"
      }
     },
     "16db2adb9be740d98494b07ba3476c12": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_917ce898bd954db6b16dfedc5bf8db69",
        "IPY_MODEL_68f7ff1f85b74198ad1ff09d313e1dd7"
       ],
       "layout": "IPY_MODEL_e52ac958abd343428e35c5509debb563"
      }
     },
     "1a003478101f4471b5ffd824ae9ae03e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "1e4b9046c48244859407f4bf9d3e127f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1f6119a94696484b99f98c42801ecd42": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "216d0356faa24fc9a165fe5c8823069a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "23b4ffc80f4f4998b723c7961568132a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "26c0288011404363ae3790b54a37b1e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "291513d41df7493bb1df4d2137fd6386": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_cad0242de9a04377b0ad7738aa52edcc",
       "style": "IPY_MODEL_827f2f470c664356babcd968c9bf5a22"
      }
     },
     "2afe0d91b3844c15a0edc43639c992e1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_358457236a554a22979f657da93a58fe",
       "max": 1,
       "style": "IPY_MODEL_3d0044f46e7947fd84935b1103b87ef4"
      }
     },
     "2dc908db477947cf996cc1b985618365": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2dfca8a1522e48b4ae99e8b6203d9a1f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2ed5a880a81948e6a315f84b3f1a7b92": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_c994c0b1bfc04175ad2845d9c5e35f4a",
       "max": 1,
       "style": "IPY_MODEL_a131c87da6974992b4a07a39a522f1c7"
      }
     },
     "3068cc511d654776baec2479326b275d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3295092c16c5426ea831f3e36409b980": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "33f8b8418346474e828880d823011766": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "358457236a554a22979f657da93a58fe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "360e0ffa23fd4b7e81ae3abf520822b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "38b914ec0f0349da95d9563d41caf753": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a7b5e28abae04d77ad407a2205d67c14",
       "style": "IPY_MODEL_5d2bd7c1ccc4496581f90db793207785",
       "value": " 28609/28609 [00:03&lt;00:00, 9324.75 examples/s]"
      }
     },
     "397d23f507e04e468ed1b5f6394c7294": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3bfa08043639424a9149d976040d7232": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3c723c8e106545c58807bc345e62a67b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f5b02d66f4c24bd483fcfdc8866d8f87",
       "style": "IPY_MODEL_9ff946b0d1b14c219ad6b1254aefe1c3",
       "value": "tokenizer.json: "
      }
     },
     "3d0044f46e7947fd84935b1103b87ef4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "3d141a0bf1ed4477bcf421657a2e4ba8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_5107c73938b949a787fadb8d14e40b16",
       "style": "IPY_MODEL_b026770d80df4b67b50a7abde8472325"
      }
     },
     "3e796aaafeb8495981b3d0da8c14ac8a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "429e427e785d40bcbd3ad1dd619e48fa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "42c7c7e88d474ec38b77dab1a85172e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_d75aae9e6edc416f8404398134a22237",
        "IPY_MODEL_44e54b23db7e42acb13b66820588a64b",
        "IPY_MODEL_ef7e92ebf6064f8bbdcef4385aab3441"
       ],
       "layout": "IPY_MODEL_94dad48ad72949f880af7459bebb5d65"
      }
     },
     "44e54b23db7e42acb13b66820588a64b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_0b5846d07b0a40f09ae7b7807fe2f8b8",
       "max": 28609,
       "style": "IPY_MODEL_79ca1a3d69ac405a991cba01ec921c1d",
       "value": 28609
      }
     },
     "45e39ed1e9c8478fa6d62fe4402a7aea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_cd780fa329c24a2dbc9363a1f3535dda",
       "max": 551,
       "style": "IPY_MODEL_12109f7556df4c999f08c3c360a9e001",
       "value": 551
      }
     },
     "46ca362ad70d45ffa21aa0d4ea1a70cd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d0c766d1f39b42ea901a123f2659eb1c",
       "style": "IPY_MODEL_0a54c1e1104a48d1a44bb6213ee97686",
       "value": "special_tokens_map.json: 100%"
      }
     },
     "4806b939cc0449268f697b01712fbdeb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_6fcec12994e744a4b5c1a2e473fa7d5a",
       "style": "IPY_MODEL_ca9f4b2ef92048ecb8f627066eeee57b",
       "value": " 7356/7356 [00:00&lt;00:00, 8936.49 examples/s]"
      }
     },
     "4a6eb4912e594b639f75a7028c67d286": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_71b5bd2100044e68a79f23c7a81e0d03",
        "IPY_MODEL_9d45d200f33b4805824e9fc8ecb36866"
       ],
       "layout": "IPY_MODEL_729ae31370b84d39ab59fbb717694989"
      }
     },
     "4a7d4c2fcb164fe588c3d3ec1e25f283": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "4ab07971e02d4051a25b07201920ba29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "4d2cf3e1cd4549a684e3e21d3ef5357d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4d7f2acef45f41c0b3287d563884868d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "4ebcc08f08ca4ec2910a653ccbbeba56": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "50e9636aab624f0c84c552eb83a61f01": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_87bc889da800498f93ca7cfa6e1dfa22",
       "max": 1,
       "style": "IPY_MODEL_9afb43e1b1cc497c8762f3c80a9d7fb7"
      }
     },
     "5107c73938b949a787fadb8d14e40b16": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "510a0e0a72cf4bd5ab8efe9191f0613f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5240f229b0d64c80ade2d16757de6215": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "53abc043dddf441ab6a067c047766214": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d88b91ef603145baa500ab6751b0c7e3",
       "style": "IPY_MODEL_7422f806c1a5410eadd0377c4853e9fa",
       "value": "Map: 100%"
      }
     },
     "53b26bb1401e4ab69748fbca46fb8415": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "552a5d4301d64acb98bc2c03002125e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_66ca391e2a0549968460b30a2de85258",
       "max": 1,
       "style": "IPY_MODEL_01ad29309aa346e2b6e4b9233ebb2118"
      }
     },
     "58cbaf7a57e54c06b5305ab9cc385884": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5965d4be9e8b4149875361165ce63eba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5a694201a1344518a3e6f4b2544da101": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_46ca362ad70d45ffa21aa0d4ea1a70cd",
        "IPY_MODEL_45e39ed1e9c8478fa6d62fe4402a7aea",
        "IPY_MODEL_e19dbe7cedc64acf8db9119c53892826"
       ],
       "layout": "IPY_MODEL_6646e8f3ae4c4f649e70c3fbbbbac01a"
      }
     },
     "5ac15def6ec749af87176d8d8fd505ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ae582cc4661c4b088ccee7caa42c03f0",
        "IPY_MODEL_feaa69575421405b9af4ef3da22cb4d6",
        "IPY_MODEL_8d3ae75fe2db46338c8e261ed8ca5227"
       ],
       "layout": "IPY_MODEL_df74245c59a140fcb51fb0723ce5694c"
      }
     },
     "5c8e5042562a456a9a43b1b1e78e85f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3068cc511d654776baec2479326b275d",
       "style": "IPY_MODEL_c30e840be8e64569a38ff038f8704cf8",
       "value": " 500k/500k [00:01&lt;00:00, 330kB/s]"
      }
     },
     "5d2bd7c1ccc4496581f90db793207785": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5df9f571941546f59b9f05f61dd4b35c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_89e10d95187146d298aea062de2299fa",
       "max": 1,
       "style": "IPY_MODEL_f3efb0bef2514342890d2e558971784d",
       "value": 1
      }
     },
     "5ec966fc755b43d1a9600d779b545e66": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "60181468d88942458333b8c29034378e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "6646e8f3ae4c4f649e70c3fbbbbac01a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "66ca391e2a0549968460b30a2de85258": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "66ee5da8d0fa4fcbb2f2bd297450130c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_7fb5058b0c00451e989c4f6eed22c2b3",
       "style": "IPY_MODEL_7cd074f2cfd0419fb7bab7fe1ce913ff"
      }
     },
     "68f7ff1f85b74198ad1ff09d313e1dd7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_df459f4758114579be17094559ed6a54",
       "max": 1,
       "style": "IPY_MODEL_8ef8a2871f894367bcd8c9700af5e016"
      }
     },
     "6b73e4a4d7664f788289c7be3360576e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_c3fddd73b0dd472b8957e1dd49af4996",
       "max": 608,
       "style": "IPY_MODEL_79f5b2bc4f6c45bdb07aafef64eb3cad",
       "value": 608
      }
     },
     "6c8c1e0d2894425790f77b9ff9ecb864": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_53b26bb1401e4ab69748fbca46fb8415",
       "style": "IPY_MODEL_97a01469f7c245e8b16f40c0882f0b73",
       "value": "tokenizer.model: 100%"
      }
     },
     "6fcec12994e744a4b5c1a2e473fa7d5a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "71b5bd2100044e68a79f23c7a81e0d03": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_1e4b9046c48244859407f4bf9d3e127f",
       "style": "IPY_MODEL_850a6e40327043f9a086ced9ba69be89"
      }
     },
     "729ae31370b84d39ab59fbb717694989": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7422f806c1a5410eadd0377c4853e9fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7453f13b6d9b4c67a5cc43dab9fe2571": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_53abc043dddf441ab6a067c047766214",
        "IPY_MODEL_a694a824cf944053bcc14dbb1e87acd0",
        "IPY_MODEL_38b914ec0f0349da95d9563d41caf753"
       ],
       "layout": "IPY_MODEL_8f40bdd272ef4040955dbcf171ea36e8"
      }
     },
     "7673016edcef4ff88c14add54643d958": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_fa8cfd5b4dee426fa40ee740c29a6ce0",
       "style": "IPY_MODEL_b676f10284c04bc3b7c892c484fb0eaf",
       "value": "config.json: 100%"
      }
     },
     "79ca1a3d69ac405a991cba01ec921c1d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "79f5b2bc4f6c45bdb07aafef64eb3cad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "7cd074f2cfd0419fb7bab7fe1ce913ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "7da1eb9bc0154a73881c1aea52b65149": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7e061ddfa1e6404d87bb5ea092baf179": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "7fb5058b0c00451e989c4f6eed22c2b3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "807d23a509bb49f984ccd1da65453a97": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_af54581a5864464dad08eb5539e454be",
       "style": "IPY_MODEL_4d2cf3e1cd4549a684e3e21d3ef5357d",
       "value": " 1.84M/? [00:00&lt;00:00, 40.8MB/s]"
      }
     },
     "827f2f470c664356babcd968c9bf5a22": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "82cf5604805945209b83a3e88e57b700": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_2dfca8a1522e48b4ae99e8b6203d9a1f",
       "style": "IPY_MODEL_fd7af03498ee42f897ee4542f07be990",
       "value": " 7356/7356 [00:00&lt;00:00, 8648.57 examples/s]"
      }
     },
     "83e20f93eb7d43b0ae60fe63eec16896": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "83f83776f0284dab88f340308c2e735c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "850a6e40327043f9a086ced9ba69be89": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "87bc889da800498f93ca7cfa6e1dfa22": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "88f648a7873c41ff96319a2d7e5ef7ac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ec141df2ab65443d99fa6c12a9f9ec25",
        "IPY_MODEL_5df9f571941546f59b9f05f61dd4b35c",
        "IPY_MODEL_8b52afeef92545f5873e5534a285d1df"
       ],
       "layout": "IPY_MODEL_3e796aaafeb8495981b3d0da8c14ac8a"
      }
     },
     "89e10d95187146d298aea062de2299fa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "20px"
      }
     },
     "8b52afeef92545f5873e5534a285d1df": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_23b4ffc80f4f4998b723c7961568132a",
       "style": "IPY_MODEL_5240f229b0d64c80ade2d16757de6215",
       "value": " 1.29k/? [00:00&lt;00:00, 80.7kB/s]"
      }
     },
     "8c9aa22a42fa49509e544cda5b2e1df2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8d29fdab0c6a4e7ab9eeeab28add236e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8d3ae75fe2db46338c8e261ed8ca5227": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_83e20f93eb7d43b0ae60fe63eec16896",
       "style": "IPY_MODEL_90f8b9fa9b344d86ac80c6b601dfd1e3",
       "value": " 4905/4905 [00:00&lt;00:00, 8880.10 examples/s]"
      }
     },
     "8dcf8a127dd6461cb740ff4881fa4989": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "8e69a3182b164856a3e4f28f91a8ccc3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8ef8a2871f894367bcd8c9700af5e016": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "8f40bdd272ef4040955dbcf171ea36e8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "90f8b9fa9b344d86ac80c6b601dfd1e3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "917ce898bd954db6b16dfedc5bf8db69": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_397d23f507e04e468ed1b5f6394c7294",
       "style": "IPY_MODEL_7e061ddfa1e6404d87bb5ea092baf179"
      }
     },
     "91a6ca4ad8614bf2b3de676e86f2b710": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "92e7aace68314bf5926305f29fd2e9cc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "93a6b4475ce44827b31b442a9c0fc329": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "945442594c424fa3b5647e1ad71a2fee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_3c723c8e106545c58807bc345e62a67b",
        "IPY_MODEL_f7b3405c5e0e41c88412248c8c49231a",
        "IPY_MODEL_807d23a509bb49f984ccd1da65453a97"
       ],
       "layout": "IPY_MODEL_f47f126a194e4855a27a3e151d634f86"
      }
     },
     "94dad48ad72949f880af7459bebb5d65": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "960bd243d7874c44a6ca77267c708733": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e6302579f72a4ae285194156e658e102",
       "style": "IPY_MODEL_b5a16008516b440ebe0a713b3fdeb1ee",
       "value": "Map: 100%"
      }
     },
     "97a01469f7c245e8b16f40c0882f0b73": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9898be97fab14d0a8f45e433cf017223": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_960bd243d7874c44a6ca77267c708733",
        "IPY_MODEL_b0884f636e69450ea75f63859a32a514",
        "IPY_MODEL_4806b939cc0449268f697b01712fbdeb"
       ],
       "layout": "IPY_MODEL_ca7c41111e7546c89b552d7fab53fb3e"
      }
     },
     "9afb43e1b1cc497c8762f3c80a9d7fb7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "9b4ecd887d8d48b9bf50cd10cd583f83": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_f9fa3d6a846641f5a4ae98e273513696",
        "IPY_MODEL_a3149b181fda4a448606a607951fe563",
        "IPY_MODEL_82cf5604805945209b83a3e88e57b700"
       ],
       "layout": "IPY_MODEL_2dc908db477947cf996cc1b985618365"
      }
     },
     "9d45d200f33b4805824e9fc8ecb36866": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_cdba3d7d62624a759845e04c8484119c",
       "max": 1,
       "style": "IPY_MODEL_26c0288011404363ae3790b54a37b1e9"
      }
     },
     "9df55266042e4bf9ade8a38ab1b680e0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9ff946b0d1b14c219ad6b1254aefe1c3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a131c87da6974992b4a07a39a522f1c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "a1edcf76f83b4beb894c678574d4bdcc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ea0671cb50f44ee993654f153060ac53",
        "IPY_MODEL_552a5d4301d64acb98bc2c03002125e6"
       ],
       "layout": "IPY_MODEL_ea580437567849f0adf529c67f1ce4e1"
      }
     },
     "a3149b181fda4a448606a607951fe563": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_429e427e785d40bcbd3ad1dd619e48fa",
       "max": 7356,
       "style": "IPY_MODEL_60181468d88942458333b8c29034378e",
       "value": 7356
      }
     },
     "a694a824cf944053bcc14dbb1e87acd0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_3bfa08043639424a9149d976040d7232",
       "max": 28609,
       "style": "IPY_MODEL_1f6119a94696484b99f98c42801ecd42",
       "value": 28609
      }
     },
     "a7b5e28abae04d77ad407a2205d67c14": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "aadb7f2e1e6246dba9b89697824d691e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "ae582cc4661c4b088ccee7caa42c03f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4ebcc08f08ca4ec2910a653ccbbeba56",
       "style": "IPY_MODEL_3295092c16c5426ea831f3e36409b980",
       "value": "Map: 100%"
      }
     },
     "af54581a5864464dad08eb5539e454be": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b026770d80df4b67b50a7abde8472325": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "b0884f636e69450ea75f63859a32a514": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_8e69a3182b164856a3e4f28f91a8ccc3",
       "max": 7356,
       "style": "IPY_MODEL_e4d4d281ada541f487be03e954a3ea16",
       "value": 7356
      }
     },
     "b5a16008516b440ebe0a713b3fdeb1ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b60da126220448fc9ccdfb427535fafc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b676f10284c04bc3b7c892c484fb0eaf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c30e840be8e64569a38ff038f8704cf8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c3fddd73b0dd472b8957e1dd49af4996": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c8c83f22ab014ebab54febc4aa8e40e6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c994c0b1bfc04175ad2845d9c5e35f4a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ca5b4e8626fc407981f081468ab0ab48": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_6c8c1e0d2894425790f77b9ff9ecb864",
        "IPY_MODEL_cba732cc5041492992f9970cc046914b",
        "IPY_MODEL_5c8e5042562a456a9a43b1b1e78e85f9"
       ],
       "layout": "IPY_MODEL_5965d4be9e8b4149875361165ce63eba"
      }
     },
     "ca7c41111e7546c89b552d7fab53fb3e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ca9f4b2ef92048ecb8f627066eeee57b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "cad0242de9a04377b0ad7738aa52edcc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cba732cc5041492992f9970cc046914b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_d2eb2da2d8d14bc6aed2cb7b9c60e03c",
       "max": 499723,
       "style": "IPY_MODEL_aadb7f2e1e6246dba9b89697824d691e",
       "value": 499723
      }
     },
     "cbafa99bc71d41c4b403af972b3c38b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_0ed1495265b94344b629fdd762854463",
       "max": 1,
       "style": "IPY_MODEL_93a6b4475ce44827b31b442a9c0fc329"
      }
     },
     "cd780fa329c24a2dbc9363a1f3535dda": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cdba3d7d62624a759845e04c8484119c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ce7701f6db684e7886e44beb4ca22ec1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d0c766d1f39b42ea901a123f2659eb1c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d236e22445c444028a4dce7f18383d84": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ea3904ac64d6421f9c7ce62627db4622",
        "IPY_MODEL_2afe0d91b3844c15a0edc43639c992e1"
       ],
       "layout": "IPY_MODEL_33f8b8418346474e828880d823011766"
      }
     },
     "d2eb2da2d8d14bc6aed2cb7b9c60e03c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d75aae9e6edc416f8404398134a22237": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9df55266042e4bf9ade8a38ab1b680e0",
       "style": "IPY_MODEL_01c7dc7dca27491ba4315951d536ddc5",
       "value": "Map: 100%"
      }
     },
     "d88b91ef603145baa500ab6751b0c7e3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "db2843366b9949b483707bf8059359c5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "dd7a81d8c78a4208a1fbdbaddedf3678": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_7673016edcef4ff88c14add54643d958",
        "IPY_MODEL_6b73e4a4d7664f788289c7be3360576e",
        "IPY_MODEL_1532dbdff0a14110bea2e49b3c208969"
       ],
       "layout": "IPY_MODEL_92e7aace68314bf5926305f29fd2e9cc"
      }
     },
     "de645524dde34a9b8f69592db8338c36": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "df459f4758114579be17094559ed6a54": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "df74245c59a140fcb51fb0723ce5694c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e19dbe7cedc64acf8db9119c53892826": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_ce7701f6db684e7886e44beb4ca22ec1",
       "style": "IPY_MODEL_360e0ffa23fd4b7e81ae3abf520822b9",
       "value": " 551/551 [00:00&lt;00:00, 31.1kB/s]"
      }
     },
     "e4d4d281ada541f487be03e954a3ea16": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "e52ac958abd343428e35c5509debb563": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e6302579f72a4ae285194156e658e102": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ea0671cb50f44ee993654f153060ac53": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_91a6ca4ad8614bf2b3de676e86f2b710",
       "style": "IPY_MODEL_5ec966fc755b43d1a9600d779b545e66"
      }
     },
     "ea3904ac64d6421f9c7ce62627db4622": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_db2843366b9949b483707bf8059359c5",
       "style": "IPY_MODEL_4d7f2acef45f41c0b3287d563884868d"
      }
     },
     "ea580437567849f0adf529c67f1ce4e1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ec141df2ab65443d99fa6c12a9f9ec25": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_05bc6c6e43474c22b1e0d05919b388fa",
       "style": "IPY_MODEL_510a0e0a72cf4bd5ab8efe9191f0613f",
       "value": "tokenizer_config.json: "
      }
     },
     "ef7e92ebf6064f8bbdcef4385aab3441": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_8d29fdab0c6a4e7ab9eeeab28add236e",
       "style": "IPY_MODEL_047a1186f5924f139e38591683e807f2",
       "value": " 28609/28609 [00:03&lt;00:00, 9014.12 examples/s]"
      }
     },
     "f3efb0bef2514342890d2e558971784d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f47f126a194e4855a27a3e151d634f86": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f5b02d66f4c24bd483fcfdc8866d8f87": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f7b3405c5e0e41c88412248c8c49231a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_0f1b6fec64ec49ff94838910dca2ec0b",
       "max": 1,
       "style": "IPY_MODEL_4ab07971e02d4051a25b07201920ba29",
       "value": 1
      }
     },
     "f9fa3d6a846641f5a4ae98e273513696": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_216d0356faa24fc9a165fe5c8823069a",
       "style": "IPY_MODEL_8c9aa22a42fa49509e544cda5b2e1df2",
       "value": "Map: 100%"
      }
     },
     "fa8cfd5b4dee426fa40ee740c29a6ce0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fa9d6dbe395f49b88e4ca7814aa28d0d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fd146739cfde40e98da486e717d46d39": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_66ee5da8d0fa4fcbb2f2bd297450130c",
        "IPY_MODEL_cbafa99bc71d41c4b403af972b3c38b4"
       ],
       "layout": "IPY_MODEL_83f83776f0284dab88f340308c2e735c"
      }
     },
     "fd7af03498ee42f897ee4542f07be990": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "feaa69575421405b9af4ef3da22cb4d6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_7da1eb9bc0154a73881c1aea52b65149",
       "max": 4905,
       "style": "IPY_MODEL_8dcf8a127dd6461cb740ff4881fa4989",
       "value": 4905
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
